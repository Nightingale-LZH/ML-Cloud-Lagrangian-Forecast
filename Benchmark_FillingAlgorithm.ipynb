{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee42a84-a031-4a4a-b562-d2044beb5a5a",
   "metadata": {},
   "source": [
    "# TCC Feedback and \"Feedforward\" Alternated front back, Filling Comparison\n",
    "**Separate Variable**\n",
    "\n",
    "For Hybrid fill, GRU threshold is 4, which means equal or more than 4 consecutive blank, GRU is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf9709-bc8e-4425-b971-ef3ba5087481",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4478f1-715c-44b7-82fe-320a66f9bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sp_stats\n",
    "import scipy.interpolate as sp_interp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import LZH_Utilities as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e529fd-fc84-4863-a5ab-4bcf6050bc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USING_STANDARD_DIVIATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc812215",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"full_rank_dataset_anormaly\"\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = pathlib.Path(\"Output/filling_comp_separate_vars_sd_metric\") if USING_STANDARD_DIVIATION else pathlib.Path(\"Output/filling_comp_separate_vars\")\n",
    "\n",
    "\n",
    "RANDOM_DATA_FOLDER_PATH = OUTPUT_FOLDER / \"filling_comparison/\"\n",
    "os.makedirs(RANDOM_DATA_FOLDER_PATH, exist_ok=True)\n",
    "RANDOM_DATA_FOLDER_ARCHIVE = utl.Archive(RANDOM_DATA_FOLDER_PATH)\n",
    "\n",
    "REALISTIC_DATA_OUTPUT_FOLDER_PATH = OUTPUT_FOLDER / \"filling_comparison_realistic/\"\n",
    "os.makedirs(REALISTIC_DATA_OUTPUT_FOLDER_PATH, exist_ok=True)\n",
    "REALISTIC_DATA_OUTPUT_FOLDER_PATH_ARCHIVE = utl.Archive(REALISTIC_DATA_OUTPUT_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3120f3-43d0-4f6c-bc54-f9a0747bd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work():\n",
    "    def save_all_data(archive:utl.Archive, time_stamp, fill_MSE, fill_MAE):\n",
    "        archive.save_data(fill_MAE, \"FILL_MAE_{0}.pkl\".format(time_stamp))\n",
    "        archive.save_data(fill_MSE, \"FILL_MSE_{0}.pkl\".format(time_stamp))\n",
    "\n",
    "    def load_data(file_name):\n",
    "        \"\"\"\n",
    "        return as dictionary that matches\n",
    "        \"\"\"\n",
    "        ret_val = {}\n",
    "        df = pd.read_csv(file_name)\n",
    "        for col_name in df.columns:\n",
    "            ret_val[col_name] = df[col_name].to_numpy()\n",
    "        return ret_val\n",
    "\n",
    "    def transpose_stack(arr):\n",
    "        return np.array([arr]).T\n",
    "\n",
    "    def tsr(arr):\n",
    "        return torch.tensor(arr)\n",
    "\n",
    "    def plot(x, y, x_label=\"\", y_label=\"\", legend=\"\", title=\"\"):\n",
    "        # plt.figure(figsize=[8, 6], dpi=300)\n",
    "        plt.figure(figsize=[8, 6])\n",
    "\n",
    "        if (type(legend) is list):\n",
    "            for yy in y:\n",
    "                plt.plot(x, yy)\n",
    "            plt.legend(legend)\n",
    "        else: \n",
    "            plt.plot(x, y)\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_diff_percentage(x, y, x_label=\"\", y_label=\"\", title=\"\"):\n",
    "        # plt.figure(figsize=[8, 6], dpi=300)\n",
    "        plt.figure(figsize=[8, 6])\n",
    "\n",
    "        x_fit = np.linspace(0, 100, 1000)\n",
    "        y_fit1 = np.polyval(np.polyfit(x, y, 1), x_fit)\n",
    "\n",
    "        plt.scatter(x, y, s=0.5, c='k')\n",
    "        plt.plot(x_fit, y_fit1, \"r\")\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend([\"linear regression\", \"data\"])\n",
    "        plt.ylim([0, 100])\n",
    "        plt.xlim([0, 100])\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "        result = sp_stats.linregress(x, y)\n",
    "        print(\"     slope: {0}\".format(result.slope))\n",
    "        print(\" intercept: {0}\".format(result.intercept))\n",
    "        print(\"corr coeff: {0}\".format(result.rvalue))\n",
    "        print(\"  variance: {0}\".format(result.rvalue ** 2))\n",
    "\n",
    "\n",
    "    def plot_diff_percentage_bined_average(x, y, x_label=\"\", y_label=\"\", title=\"\", bin_count=10):\n",
    "        # plt.figure(figsize=[8, 6], dpi=300)\n",
    "        plt.figure(figsize=[8, 6])\n",
    "\n",
    "        # Overall\n",
    "        x_fit = np.linspace(0, 100, 10)\n",
    "        y_fit1 = np.polyval(np.polyfit(x, y, 1), x_fit)\n",
    "\n",
    "        plt.scatter(x, y, s=0.5, alpha=0.3, c='k', label=\"data\")\n",
    "        plt.plot(x_fit, y_fit1, \"r\", label=\"linear regression\")\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "\n",
    "        result = sp_stats.linregress(x, y)\n",
    "        print(\"Overall:\")\n",
    "        print(\"     slope: {0}\".format(result.slope))\n",
    "        print(\" intercept: {0}\".format(result.intercept))\n",
    "        print(\"corr coeff: {0}\".format(result.rvalue))\n",
    "        print(\"  variance: {0}\".format(result.rvalue ** 2))\n",
    "        print()\n",
    "\n",
    "        # Binned Average\n",
    "        is_label_printed = False\n",
    "\n",
    "        bin_arr = np.linspace(0, 100, bin_count+1)\n",
    "        for (low, high) in zip(bin_arr[:-1], bin_arr[1:]):\n",
    "\n",
    "            bin_filter = np.logical_and(low <= x, x <= high)\n",
    "            x_bin = x[bin_filter]\n",
    "            y_bin = y[bin_filter]\n",
    "\n",
    "            x_fit = np.linspace(low, high, 10)\n",
    "            y_fit1 = np.polyval(np.polyfit(x_bin, y_bin, 1), x_fit)\n",
    "\n",
    "            if (not is_label_printed):\n",
    "                plt.plot(x_fit, y_fit1, \"b\", label=\"binned linear reg.\")\n",
    "                is_label_printed = True\n",
    "            else:\n",
    "                plt.plot(x_fit, y_fit1, \"b\")\n",
    "\n",
    "            result = sp_stats.linregress(x_bin, y_bin)\n",
    "            print(\"[{4:>3}, {5:>3}):   b:{0:.4f};  m:{1:.4f}; r:{2:.4f}; var:{3:.4f}\".format(result.slope, result.intercept, result.rvalue, result.rvalue ** 2, int(low), int(high)))\n",
    "\n",
    "\n",
    "        plt.legend()\n",
    "        plt.ylim([0, 100])\n",
    "        plt.xlim([0, 100])\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def flatten_time_axis(data):\n",
    "        \"\"\"\n",
    "        flatten the time axis so it could be used in FC and linear model. \n",
    "        \"\"\"\n",
    "        return np.concatenate(data, axis=0)\n",
    "\n",
    "    ## Data\n",
    "\n",
    "    df = utl.read_time_series_data(\"full_rank_dataset\")\n",
    "\n",
    "    for col_name in ['LTS', 'SST', 'Subsidence', 'Night_Day', 'RH', 'q', 'wsp', 'TCC']:\n",
    "        df[-1][col_name + \"_FF\"] = 0\n",
    "        for idx in np.arange(8):\n",
    "            df[idx][col_name + \"_FF\"] = df[idx + 1][col_name]\n",
    "\n",
    "    df[0][\"TCC_FB\"] = 0\n",
    "    for idx in np.arange(8):\n",
    "        df[idx + 1][\"TCC_FB\"] = df[idx][\"TCC\"]\n",
    "\n",
    "    input_col = ['LTS', 'SST', 'Subsidence', 'Night_Day', 'RH', 'q', 'wsp', \"TCC_FB\"]\n",
    "    output_col = ['LTS_FF', 'SST_FF', 'Subsidence_FF', 'Night_Day_FF', 'RH_FF', 'q_FF', 'wsp_FF', \"TCC\"]\n",
    "\n",
    "    idx_test_set = np.random.choice(np.arange(df[0].shape[0]), [int(0.1 * df[0].shape[0])], False)\n",
    "    temp = np.delete(np.arange(df[0].shape[0]), idx_test_set)\n",
    "    idx_final_test_set = np.random.choice(temp.shape[0], [int(0.1 * df[0].shape[0])], False)\n",
    "    idx_training_set = np.delete(temp, idx_final_test_set)\n",
    "\n",
    "    # print(\"Training: {0}; Testing: {1}; FinalTesting: {2}\".format(idx_training_set.shape[0], idx_test_set.shape[0], idx_final_test_set.shape[0]))\n",
    "\n",
    "    time_arr = np.arange(9)\n",
    "\n",
    "    X_full = [df[time][input_col].to_numpy() for time in time_arr]\n",
    "    y_hat_full = [np.c_[df[time][output_col].to_numpy()] for time in time_arr]\n",
    "\n",
    "    X_train = np.array([X_full[time][idx_training_set] for time in time_arr])\n",
    "    y_hat_train = np.array([y_hat_full[time][idx_training_set] for time in time_arr])\n",
    "\n",
    "    X_test = np.array([X_full[time][idx_test_set] for time in time_arr])\n",
    "    y_hat_test = np.array([y_hat_full[time][idx_test_set] for time in time_arr])\n",
    "\n",
    "    X_final_test = np.array([X_full[time][idx_final_test_set] for time in time_arr])\n",
    "    y_hat_final_test = np.array([y_hat_full[time][idx_final_test_set] for time in time_arr])\n",
    "\n",
    "    X_train_RRR     = X_train     [::-1].copy()\n",
    "    y_hat_train_RRR = y_hat_train [::-1].copy()\n",
    "\n",
    "    # The following is complicated, we are basically back in time. Pls draw the graph and confirm this is correct. \n",
    "    X_test_RRR      = y_hat_test  [::-1].copy()\n",
    "    y_hat_test_RRR  = X_test      [::-1].copy()\n",
    "\n",
    "    X_final_test_RRR     = y_hat_final_test  [::-1].copy()\n",
    "    y_hat_final_test_RRR = X_final_test      [::-1].copy()\n",
    "\n",
    "    ## Filling Model\n",
    "\n",
    "    class MLTCC_Model_GRU_Normalized:\n",
    "        def __init__(self, layer_overwrite=None):\n",
    "            self.NMR = utl.Normalizer()\n",
    "\n",
    "            self.input_size = 8\n",
    "            self.output_size = 8\n",
    "            self.batch_size = 10000\n",
    "            self.step_size = 1e-3\n",
    "\n",
    "            h_gru = 50\n",
    "            h1 = 20\n",
    "            h2 = 40\n",
    "            h3 = 20\n",
    "\n",
    "            if (layer_overwrite is not None):\n",
    "                (h_gru, h1, h2, h3) = layer_overwrite\n",
    "\n",
    "            self.gru = nn.GRU(self.input_size, h_gru, 2, batch_first=False)   # (seq, batch, feature)\n",
    "            self.fc1 = nn.Linear(h_gru, h1)\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.fc3 = nn.Linear(h2, h3)\n",
    "            self.fc4 = nn.Linear(h3, self.output_size)\n",
    "\n",
    "            variable_list = [\n",
    "                    self.fc1.weight, self.fc1.bias,\n",
    "                    self.fc2.weight, self.fc2.bias,\n",
    "                    self.fc3.weight, self.fc3.bias,\n",
    "                    self.fc4.weight, self.fc4.bias\n",
    "                ]\n",
    "            for all_weights in self.gru.all_weights:\n",
    "                for weights in all_weights:\n",
    "                    variable_list.append(weights)\n",
    "\n",
    "\n",
    "            self.optim = torch.optim.Adam(\n",
    "                variable_list,\n",
    "                lr=self.step_size\n",
    "            )\n",
    "\n",
    "            # Step function\n",
    "            self.step_h_last = None\n",
    "            self.step_y_hat_last = None\n",
    "            self.step_last_DayNight = None\n",
    "\n",
    "        def foward(self, X):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "    #         seq_X_result = []\n",
    "    #         for seq_X in X:\n",
    "    #             seq_X_result.append(torch.relu(self.fc1(seq_X.float())))\n",
    "    #         seq_X_result = tsr(seq_X_result)\n",
    "            # normalize\n",
    "            newX = []\n",
    "            for idx in np.arange(X.shape[0]):\n",
    "                newX.append(self.NMR.normalize_input(X[idx]).detach().numpy())\n",
    "            X = torch.tensor(np.array(newX))\n",
    "\n",
    "            X = X.float()\n",
    "\n",
    "            X_gru_outputs, hn = self.gru(X)\n",
    "            X = torch.relu(X_gru_outputs[-1]) # use last output\n",
    "\n",
    "            X = torch.relu(self.fc1(X))\n",
    "            X = torch.relu(self.fc2(X))\n",
    "            X = torch.relu(self.fc3(X))\n",
    "            X = self.fc4(X)\n",
    "\n",
    "            return self.NMR.denormalize_output(X)\n",
    "\n",
    "        def step_forward(self, X_step=None):\n",
    "            \"\"\"\n",
    "            X_step is in numpy.array in (n, 8)\n",
    "            reutrn value is in numpy.array in (n, 8)\n",
    "\n",
    "            if X_step is None, the algorithm will use the previous result as input\n",
    "            Any np.nan value in X_step will be replaced by corrisponding previous result. \n",
    "            \"\"\"\n",
    "            output_tensor = None\n",
    "            if (X_step is None):\n",
    "                if (self.step_h_last is None):\n",
    "                    # we do not have previous result\n",
    "                    return None\n",
    "                else:\n",
    "                    # specify the implied DayNight data\n",
    "                    # self.step_last_DayNight = 1 - self.step_last_DayNight # flip DayNight\n",
    "                    self.step_last_DayNight = np.where(self.step_last_DayNight > 0.5, 0, 1)\n",
    "                    self.step_y_hat_last[:, 3] = self.step_last_DayNight \n",
    "\n",
    "                    input_tensor = torch.tensor(np.array([self.NMR.normalize_input_nparray(self.step_y_hat_last)])).float()\n",
    "                    output_tensor, self.step_h_last = self.gru(input_tensor, self.step_h_last)\n",
    "            else:            \n",
    "                if (self.step_h_last is None):\n",
    "                    # we do not have previous result\n",
    "                    input_tensor = torch.tensor(np.array([self.NMR.normalize_input_nparray(X_step)])).float()\n",
    "                    output_tensor, self.step_h_last = self.gru(input_tensor)\n",
    "                else:\n",
    "                    for i0 in np.arange(X_step.shape[0]):\n",
    "                        for i1 in np.arange(X_step.shape[1]):\n",
    "                            if (np.isnan(X_step[i0, i1])):\n",
    "                                X_step[i0, i1] = self.step_y_hat_last[i0, i1]\n",
    "                    input_tensor = torch.tensor(np.array([self.NMR.normalize_input_nparray(X_step)])).float()\n",
    "                    output_tensor, self.step_h_last = self.gru(input_tensor, self.step_h_last)\n",
    "\n",
    "                # record last DayNight data\n",
    "                self.step_last_DayNight = X_step[:, 3]\n",
    "\n",
    "            X = torch.relu(output_tensor[-1]) # use last output\n",
    "\n",
    "            X = torch.relu(self.fc1(X))\n",
    "            X = torch.relu(self.fc2(X))\n",
    "            X = torch.relu(self.fc3(X))\n",
    "            X = self.fc4(X)\n",
    "\n",
    "            self.step_y_hat_last = self.NMR.denormalize_output(X).detach().numpy()\n",
    "\n",
    "            return self.step_y_hat_last\n",
    "\n",
    "        def step_reset(self):\n",
    "            self.step_h_last = None\n",
    "            self.step_y_hat_last = None\n",
    "            return\n",
    "\n",
    "        def loss(self, y, y_hat):\n",
    "            \"\"\"\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "            return F.mse_loss(\n",
    "                self.NMR.normalize_input(y).float(), \n",
    "                self.NMR.normalize_input(y_hat).float()\n",
    "            )\n",
    "\n",
    "        def train(self, X, y_hat, max_epoch):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "            sample_count = X.shape[1]\n",
    "\n",
    "            epoch_num = []\n",
    "            losses = []\n",
    "\n",
    "            for i in np.arange(max_epoch):\n",
    "                for idx_batch in np.arange(int(sample_count / self.batch_size))* self.batch_size:\n",
    "                    X_batch = torch.tensor(X[:, idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_batch = self.foward(X_batch)\n",
    "                    loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "                X_batch = torch.tensor(X[:, idx_rand, :]).float()\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_rand, :]).float()\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                epoch_num.append(i)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            return (epoch_num, losses)\n",
    "\n",
    "\n",
    "        def train_time_series(self, X, y_hat, max_epoch, max_epoch_per_time):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "\n",
    "            progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "\n",
    "            # setting normalizer\n",
    "            # flatten X and y_hat\n",
    "            X_flat = np.concatenate(X, axis=0)\n",
    "            y_hat_flat = np.concatenate(y_hat, axis=0)\n",
    "\n",
    "            self.NMR.set_mean_and_sd(torch.tensor(X_flat), torch.tensor(y_hat_flat))\n",
    "\n",
    "\n",
    "            full_losses = np.array([])\n",
    "\n",
    "            for i in np.arange(max_epoch):\n",
    "                for time in np.arange(X.shape[0]):\n",
    "                    (x_axis, losses) = self.train(X[:time+1, :, :], y_hat[time, :, :], max_epoch_per_time)\n",
    "                    full_losses = np.concatenate((full_losses, losses))\n",
    "\n",
    "                progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "\n",
    "            progress_bar.update().end()\n",
    "\n",
    "            return (np.arange(len(full_losses)) + 1, full_losses)\n",
    "\n",
    "    # forward training data\n",
    "    X_train_A = X_train[:-1].copy()\n",
    "    y_hat_train_A = y_hat_train[:-1].copy()\n",
    "    # [:-1] is due to feedforward process\n",
    "\n",
    "    # backward traning data\n",
    "    X_train_RRR_A = X_train_RRR[1:-1].copy()\n",
    "    y_hat_train_RRR_A = y_hat_train_RRR[1:-1].copy()\n",
    "    # [1:-1] is due to feedforward process\n",
    "\n",
    "    ### Forward Training\n",
    "\n",
    "    tcc_model_gru = MLTCC_Model_GRU_Normalized()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_gru.train_time_series(X_train_A, y_hat_train_A, 50, 2)\n",
    "\n",
    "    ### Backward Training\n",
    "\n",
    "    # tcc_model_gru_RRR = MLTCC_Model_GRU_Normalized(layer_overwrite=[10, 15, 10, 10])\n",
    "    tcc_model_gru_RRR = MLTCC_Model_GRU_Normalized()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_gru_RRR.train_time_series(X_train_RRR_A, y_hat_train_RRR_A, 50, 2)\n",
    "\n",
    "    ## Filling Function\n",
    "\n",
    "    def idx_full_rank(nparr):\n",
    "        return np.sum(np.sum(np.logical_not(np.isnan(nparr)), axis=2), axis=0) == 9 * 8\n",
    "    def count_full_rank(nparr):\n",
    "        return np.count_nonzero(idx_full_rank(nparr))\n",
    "\n",
    "    def GRU_fill(nparr_fill, tcc_model_gru, tcc_model_gru_RRR, show_progress=True):\n",
    "        nparr_fill = nparr_fill.copy()\n",
    "        filling_switching_threshold = 6\n",
    "\n",
    "        if (show_progress):\n",
    "            print(\"Full Rank Data: {0}\".format(count_full_rank(nparr_fill)))\n",
    "\n",
    "        # ------- forward filling -------\n",
    "        good_data_mask = (np.sum(np.logical_not(np.isnan(nparr_fill)), axis=2) >= 8)\n",
    "\n",
    "        # undefined indices are set to negative\n",
    "        first_good_idx = np.zeros(good_data_mask.shape[1]) - 1 # set to -1\n",
    "\n",
    "        for i1 in np.arange(good_data_mask.shape[0]):\n",
    "            mask = first_good_idx < 0\n",
    "            first_good_idx[mask] = (good_data_mask[i1][mask] * 2 - 1) * (i1 + 1) - 1\n",
    "\n",
    "        # filling\n",
    "        nparr_fill_res = nparr_fill.copy()\n",
    "\n",
    "        for starting_idx in np.arange(filling_switching_threshold):  # 0 -> filling_switching_threshold(excl.)\n",
    "            mask = first_good_idx == starting_idx\n",
    "\n",
    "            if (show_progress):\n",
    "                progress_bar = utl.TimedProgressBar(8 - starting_idx).update_msg(\"{0} forward pass, total {1}\".format(starting_idx, np.count_nonzero(mask))).update()\n",
    "\n",
    "            tcc_model_gru.step_reset()\n",
    "            for time in np.arange(starting_idx, 8):\n",
    "                nparr_fill_res[time + 1, mask] = tcc_model_gru.step_forward(nparr_fill[time, mask])\n",
    "\n",
    "                if (show_progress):\n",
    "                    progress_bar.inc_val().update()\n",
    "            if (show_progress):\n",
    "                print()\n",
    "\n",
    "        # update\n",
    "        if (show_progress):\n",
    "            print(\"Full Rank Data: {0} -> {1}\".format(count_full_rank(nparr_fill), count_full_rank(nparr_fill_res)))\n",
    "        update_mask = np.isnan(nparr_fill)\n",
    "        nparr_fill[update_mask] = nparr_fill_res[update_mask]\n",
    "\n",
    "        # ------- backward filling -------\n",
    "        good_data_mask = (np.sum(np.logical_not(np.isnan(nparr_fill)), axis=2) >= 8)\n",
    "\n",
    "        # undefined indices are set to negative\n",
    "        first_good_idx = np.zeros(good_data_mask.shape[1]) - 1 # set to -1\n",
    "\n",
    "        for i1 in np.arange(good_data_mask.shape[0])[::-1]:\n",
    "            first_good_idx[first_good_idx < 0] = (good_data_mask[i1][first_good_idx < 0] * 2 - 1) * (i1 + 1) - 1\n",
    "\n",
    "        # filling\n",
    "        nparr_fill_res = nparr_fill.copy()\n",
    "\n",
    "        for starting_idx in np.arange(filling_switching_threshold, 9):  # filling_switching_threshold(incl.) -> 8\n",
    "            mask = first_good_idx == starting_idx\n",
    "\n",
    "            if (show_progress):\n",
    "                progress_bar = utl.TimedProgressBar(np.arange(starting_idx, 0, -1).shape[0]).update_msg(\"{0} backward pass, total {1}\".format(starting_idx, np.count_nonzero(mask))).update()\n",
    "\n",
    "            tcc_model_gru_RRR.step_reset()\n",
    "            for time in np.arange(starting_idx, 0, -1):\n",
    "                nparr_fill_res[time - 1, mask] = tcc_model_gru_RRR.step_forward(nparr_fill[time, mask])\n",
    "\n",
    "                if (show_progress):\n",
    "                    progress_bar.inc_val().update()\n",
    "            if (show_progress):\n",
    "                print()\n",
    "\n",
    "\n",
    "        # update\n",
    "        if (show_progress):\n",
    "            print(\"Full Rank Data: {0} -> {1}\".format(count_full_rank(nparr_fill), count_full_rank(nparr_fill_res)))\n",
    "        update_mask = np.isnan(nparr_fill)\n",
    "        nparr_fill[update_mask] = nparr_fill_res[update_mask]\n",
    "\n",
    "        # ------- forward filling -------\n",
    "        good_data_mask = (np.sum(np.logical_not(np.isnan(nparr_fill)), axis=2) >= 8)\n",
    "\n",
    "        # undefined indices are set to negative\n",
    "        first_good_idx = np.zeros(good_data_mask.shape[1]) - 1 # set to -1\n",
    "\n",
    "        for i1 in np.arange(good_data_mask.shape[0]):\n",
    "            mask = first_good_idx < 0\n",
    "            first_good_idx[mask] = (good_data_mask[i1][mask] * 2 - 1) * (i1 + 1) - 1\n",
    "\n",
    "        # filling\n",
    "        nparr_fill_res = nparr_fill.copy()\n",
    "\n",
    "        for starting_idx in np.arange(9-1):  # 0 -> 7\n",
    "            mask = first_good_idx == starting_idx\n",
    "\n",
    "            if (show_progress):\n",
    "                progress_bar = utl.TimedProgressBar(8 - starting_idx).update_msg(\"{0} forward pass, total {1}\".format(starting_idx, np.count_nonzero(mask))).update()\n",
    "\n",
    "            tcc_model_gru.step_reset()\n",
    "            for time in np.arange(starting_idx, 8):\n",
    "                nparr_fill_res[time + 1, mask] = tcc_model_gru.step_forward(nparr_fill[time, mask])\n",
    "\n",
    "                if (show_progress):\n",
    "                    progress_bar.inc_val().update()\n",
    "            if (show_progress):\n",
    "                print()\n",
    "\n",
    "        # update\n",
    "        if (show_progress):\n",
    "            print(\"Full Rank Data: {0} -> {1}\".format(count_full_rank(nparr_fill), count_full_rank(nparr_fill_res)))\n",
    "        update_mask = np.isnan(nparr_fill)\n",
    "        nparr_fill[update_mask] = nparr_fill_res[update_mask]\n",
    "\n",
    "        if (show_progress):\n",
    "            print(\"RESULT: Full Rank Data: {0}\".format(count_full_rank(nparr_fill)))\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def day_night_only_fill_helper(nparr_fill):\n",
    "        \"\"\"\n",
    "        This help fill the day and night data for each sample\n",
    "        If the sample does not have clue, then it will be randomly assigned. \n",
    "        \"\"\"\n",
    "        nparr_fill = nparr_fill.copy()\n",
    "        day_night_idx = 3\n",
    "\n",
    "        # creating template\n",
    "        template_day_night = []\n",
    "        for i0 in np.arange(nparr_fill.shape[0]):\n",
    "            template_day_night.append(i0 % 2)\n",
    "        template_night_day = np.array(template_day_night)\n",
    "        template_day_night = 1 - template_night_day\n",
    "\n",
    "        # fill\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            if (np.count_nonzero(np.isnan(nparr_fill[:, i1, day_night_idx])) < nparr_fill.shape[0]):\n",
    "                # there are some data to interpolate\n",
    "                for i0 in np.arange(nparr_fill.shape[0]):\n",
    "                    if (not np.isnan(nparr_fill[i0, i1, day_night_idx])):\n",
    "                        if (int(nparr_fill[i0, i1, day_night_idx]) == i0 % 2):\n",
    "                            nparr_fill[:, i1, day_night_idx] = template_night_day\n",
    "                        else:\n",
    "                            nparr_fill[:, i1, day_night_idx] = template_day_night\n",
    "                        break\n",
    "            else:\n",
    "                # no data avaliable -> randomize\n",
    "                if (np.random.rand() > 0.5):\n",
    "                    nparr_fill[:, i1, day_night_idx] = template_night_day\n",
    "                else:\n",
    "                    nparr_fill[:, i1, day_night_idx] = template_day_night\n",
    "\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def mean_fill(nparr_fill):\n",
    "        \"\"\"\n",
    "        Fill data using mean value of each column\n",
    "        \"\"\"\n",
    "        nparr_fill = day_night_only_fill_helper(nparr_fill).copy()\n",
    "\n",
    "        mean = np.nanmean(np.nanmean(nparr_fill, axis=1), axis=0)\n",
    "        for i0 in np.arange(nparr_fill.shape[0]):\n",
    "            for i1 in np.arange(nparr_fill.shape[1]):\n",
    "                for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                    if (np.isnan(nparr_fill[i0, i1, i2])):\n",
    "                        nparr_fill[i0, i1, i2] = mean[i2]\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def daily_mean_fill(nparr_fill):\n",
    "        \"\"\"\n",
    "        Fill data using mean value of each day and each column\n",
    "        \"\"\"\n",
    "        nparr_fill = day_night_only_fill_helper(nparr_fill).copy()\n",
    "\n",
    "        mean = np.nanmean(nparr_fill, axis=1)\n",
    "        for i0 in np.arange(nparr_fill.shape[0]):\n",
    "            for i1 in np.arange(nparr_fill.shape[1]):\n",
    "                for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                    if (np.isnan(nparr_fill[i0, i1, i2])):\n",
    "                        nparr_fill[i0, i1, i2] = mean[i0, i2]\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def day_night_mean_fill(nparr_fill):\n",
    "        \"\"\"\n",
    "        Fill data using day or night mean value of each column \n",
    "        \"\"\"\n",
    "        nparr_fill = day_night_only_fill_helper(nparr_fill).copy()\n",
    "\n",
    "        day_night_idx = 3\n",
    "\n",
    "        day_mean = np.nanmean(nparr_fill[nparr_fill[:, :, day_night_idx] > 0.5], axis=0)\n",
    "        night_mean = np.nanmean(nparr_fill[nparr_fill[:, :, day_night_idx] < 0.5], axis=0)\n",
    "        for i0 in np.arange(nparr_fill.shape[0]):\n",
    "            for i1 in np.arange(nparr_fill.shape[1]):\n",
    "                for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                    if (np.isnan(nparr_fill[i0, i1, i2])):\n",
    "                        if (nparr_fill[i0, i1, day_night_idx] > 0.5):\n",
    "                            nparr_fill[i0, i1, i2] = day_mean[i2]\n",
    "                        else:\n",
    "                            nparr_fill[i0, i1, i2] = night_mean[i2]\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def linear_fill(nparr_fill):\n",
    "        \"\"\"\n",
    "        Fill data using linear interpolation along time. \n",
    "        (mean value will be used if there no useful data)\n",
    "        \"\"\"\n",
    "        def linear_fill_sample(sample, mean):\n",
    "            if (np.count_nonzero(np.isnan(sample)) >= sample.shape[0]):\n",
    "                # There is no data for the entire sample\n",
    "                sample = np.ones(sample.shape) * mean\n",
    "            else:\n",
    "                # linear interpolation\n",
    "                prev_idx = -1\n",
    "                for curr_idx in np.arange(sample.shape[0]):\n",
    "                    if (not np.isnan(sample[curr_idx])):\n",
    "                        if (prev_idx != curr_idx):\n",
    "                            # need some filling to do\n",
    "                            if (prev_idx == -1):\n",
    "                                # no begining data yet\n",
    "                                sample[:curr_idx] = sample[curr_idx]\n",
    "                            else:\n",
    "                                # linear interpolation\n",
    "                                sample[prev_idx:curr_idx] = np.polyval(\n",
    "                                    np.polyfit([prev_idx, curr_idx],[sample[prev_idx], sample[curr_idx]], 1), \n",
    "                                    np.arange(prev_idx, curr_idx)\n",
    "                                )\n",
    "\n",
    "                        prev_idx = curr_idx\n",
    "                if (prev_idx != sample.shape[0] - 1):\n",
    "                    # need some filing at the end\n",
    "                    sample[prev_idx:] = sample[prev_idx]\n",
    "\n",
    "            return sample\n",
    "\n",
    "\n",
    "        nparr_fill = day_night_only_fill_helper(nparr_fill).copy()\n",
    "\n",
    "        day_night_idx = 3\n",
    "\n",
    "        mean = np.nanmean(np.nanmean(nparr_fill, axis=1), axis=0) # for no data file. \n",
    "\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                nparr_fill[:, i1, i2] = linear_fill_sample(nparr_fill[:, i1, i2], mean[i2])\n",
    "\n",
    "        return nparr_fill \n",
    "\n",
    "    def day_night_linear_fill(nparr_fill):\n",
    "        \"\"\"\n",
    "        Fill data using linear interpolation along time. \n",
    "        (mean value will be used if there no useful data)\n",
    "        \"\"\"\n",
    "        def linear_fill_sample(sample, mean):\n",
    "            if (np.count_nonzero(np.isnan(sample)) >= sample.shape[0]):\n",
    "                # There is no data for the entire sample\n",
    "                sample = np.ones(sample.shape) * mean\n",
    "            else:\n",
    "                # linear interpolation\n",
    "                prev_idx = -1\n",
    "                for curr_idx in np.arange(sample.shape[0]):\n",
    "                    if (not np.isnan(sample[curr_idx])):\n",
    "                        if (prev_idx != curr_idx):\n",
    "                            # need some filling to do\n",
    "                            if (prev_idx == -1):\n",
    "                                # no begining data yet\n",
    "                                sample[:curr_idx] = sample[curr_idx]\n",
    "                            else:\n",
    "                                # linear interpolation\n",
    "                                sample[prev_idx:curr_idx] = np.polyval(\n",
    "                                    np.polyfit([prev_idx, curr_idx],[sample[prev_idx], sample[curr_idx]], 1), \n",
    "                                    np.arange(prev_idx, curr_idx)\n",
    "                                )\n",
    "\n",
    "                        prev_idx = curr_idx\n",
    "                if (prev_idx != sample.shape[0] - 1):\n",
    "                    # need some filing at the end\n",
    "                    sample[prev_idx:] = sample[prev_idx]\n",
    "\n",
    "            return sample\n",
    "\n",
    "\n",
    "        nparr_fill = day_night_only_fill_helper(nparr_fill).copy()\n",
    "\n",
    "        day_night_idx = 3\n",
    "\n",
    "        day_mean = np.nanmean(nparr_fill[nparr_fill[:, :, day_night_idx] > 0.5], axis=0)\n",
    "        night_mean = np.nanmean(nparr_fill[nparr_fill[:, :, day_night_idx] < 0.5], axis=0) \n",
    "\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                day_mask = nparr_fill[:, i1, day_night_idx] > 0.5\n",
    "                night_mask = nparr_fill[:, i1, day_night_idx] < 0.5\n",
    "\n",
    "                nparr_fill[:, i1, i2][day_mask] = linear_fill_sample(nparr_fill[:, i1, i2][day_mask], day_mean[i2])\n",
    "                nparr_fill[:, i1, i2][night_mask] = linear_fill_sample(nparr_fill[:, i1, i2][night_mask], night_mean[i2])\n",
    "\n",
    "        return nparr_fill \n",
    "\n",
    "    def cubic_spline_fill(nparr_fill):\n",
    "        def cubic_spline_sample(sample, mean):\n",
    "            timestamp = []\n",
    "            avaliable_data = []\n",
    "            for (time, data) in zip(np.arange(sample.shape[0]), sample):\n",
    "                if (not np.isnan(data)):\n",
    "                    timestamp.append(time)\n",
    "                    avaliable_data.append(data)\n",
    "            timestamp = np.array(timestamp)\n",
    "            avaliable_data = np.array(avaliable_data)\n",
    "\n",
    "            x = np.concatenate(([-1], timestamp, [9]))\n",
    "            y = np.concatenate(([mean], avaliable_data, [mean]))\n",
    "            if (len(x) > 3):\n",
    "                return sp_interp.interp1d(x, y, kind=\"cubic\")(np.arange(9))\n",
    "            else:\n",
    "                return sp_interp.interp1d(x, y, kind=\"quadratic\")(np.arange(9))\n",
    "\n",
    "        nparr_fill = nparr_fill.copy()\n",
    "        mean = np.nanmean(np.nanmean(nparr_fill, axis=1), axis=0)\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                nparr_fill[:, i1, i2] = cubic_spline_sample(nparr_fill[:, i1, i2], mean[i2])\n",
    "        return nparr_fill\n",
    "\n",
    "    def GRU_linear_hybrid_mux_fill(nparr_fill, GRU_threshold=4, tcc_model_gru_fill=None, tcc_model_gru_RRR_fill=None):\n",
    "        \"\"\"\n",
    "        This function will calculate the GRU fill and linear fill of both function, \n",
    "            then determine which one will be in the final result.\n",
    "\n",
    "        GRU_threshold: int, default as 2\n",
    "            if there are n consecutive blanks in the sample\n",
    "            where n > GRU_threshold, GRU filled sample will be chosen to fill the \n",
    "            final result\n",
    "        \"\"\"\n",
    "\n",
    "        def GRU_linear_hybrid_fill_sample(sample, GRU_sample, linear_sample):\n",
    "            x = sample\n",
    "\n",
    "            # making a mask of fill types\n",
    "            #   0 -> current is not a blank\n",
    "            #   # -> sum # consecutive blanks before, after the idx and self\n",
    "            x_mask = np.isnan(x).astype('int64')\n",
    "\n",
    "            # always points to the index of blank starts - 1\n",
    "            idx_start = -1\n",
    "            for idx in np.arange(x_mask.shape[0]):\n",
    "                if (x_mask[idx] == 0): # no blank at current idx\n",
    "                    if (idx - idx_start > 2): # filling needed in between\n",
    "                        x_mask[idx_start+1:idx] = idx - idx_start - 1\n",
    "                    idx_start = idx\n",
    "            idx += 1\n",
    "            if (idx - idx_start > 2): # filling needed in between\n",
    "                x_mask[idx_start+1:] = idx - idx_start - 1\n",
    "\n",
    "            # filling\n",
    "            for idx in np.arange(x.shape[0]):\n",
    "                if (0 < x_mask[idx] and x_mask[idx] < GRU_threshold):\n",
    "                    x[idx] = linear_sample[idx]\n",
    "                elif (x_mask[idx] >= GRU_threshold):\n",
    "                    x[idx] = GRU_sample[idx]\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "        if (tcc_model_gru_fill is None): \n",
    "            tcc_model_gru_fill = tcc_model_gru\n",
    "\n",
    "        if (tcc_model_gru_RRR_fill is None): \n",
    "            tcc_model_gru_RRR_fill = tcc_model_gru_RRR\n",
    "\n",
    "        nparr_fill_GRU = GRU_fill(nparr_fill.copy(), tcc_model_gru_fill, tcc_model_gru_RRR_fill, show_progress=False)\n",
    "        nparr_fill_linear = linear_fill(nparr_fill.copy())\n",
    "\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                nparr_fill[:, i1, i2] = GRU_linear_hybrid_fill_sample(\n",
    "                    nparr_fill[:, i1, i2],\n",
    "                    nparr_fill_GRU[:, i1, i2],\n",
    "                    nparr_fill_linear[:, i1, i2]\n",
    "                )\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    def GRU_linear_hybrid_fill(nparr_fill, GRU_threshold=4, tcc_model_gru_fill=None, tcc_model_gru_RRR_fill=None):\n",
    "        \"\"\"\n",
    "        This function will will use GRU fill based on the linear filled sample\n",
    "\n",
    "        GRU_threshold: int, default as 2\n",
    "            if there are n consecutive blanks in the sample\n",
    "            where n > GRU_threshold, GRU filled sample will be chosen to fill the \n",
    "            final result\n",
    "        \"\"\"\n",
    "\n",
    "        def GRU_linear_hybrid_fill_sample(sample, linear_sample):\n",
    "            x = sample\n",
    "\n",
    "            # making a mask of fill types\n",
    "            #   0 -> current is not a blank\n",
    "            #   # -> sum # consecutive blanks before, after the idx and self\n",
    "            x_mask = np.isnan(x).astype('int64')\n",
    "\n",
    "            # always points to the index of blank starts - 1\n",
    "            idx_start = -1\n",
    "            for idx in np.arange(x_mask.shape[0]):\n",
    "                if (x_mask[idx] == 0): # no blank at current idx\n",
    "                    if (idx - idx_start > 2): # filling needed in between\n",
    "                        x_mask[idx_start+1:idx] = idx - idx_start - 1\n",
    "                    idx_start = idx\n",
    "            idx += 1\n",
    "            if (idx - idx_start > 2): # filling needed in between\n",
    "                x_mask[idx_start+1:] = idx - idx_start - 1\n",
    "\n",
    "            # filling\n",
    "            for idx in np.arange(x.shape[0]):\n",
    "                if (0 < x_mask[idx] and x_mask[idx] < GRU_threshold):\n",
    "                    x[idx] = linear_sample[idx]\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "        if (tcc_model_gru_fill is None): \n",
    "            tcc_model_gru_fill = tcc_model_gru\n",
    "\n",
    "        if (tcc_model_gru_RRR_fill is None): \n",
    "            tcc_model_gru_RRR_fill = tcc_model_gru_RRR\n",
    "\n",
    "        nparr_fill_linear = linear_fill(nparr_fill.copy())\n",
    "\n",
    "        for i1 in np.arange(nparr_fill.shape[1]):\n",
    "            for i2 in np.arange(nparr_fill.shape[2]):\n",
    "                nparr_fill[:, i1, i2] = GRU_linear_hybrid_fill_sample(\n",
    "                    nparr_fill[:, i1, i2],\n",
    "                    nparr_fill_linear[:, i1, i2]\n",
    "                )\n",
    "\n",
    "        nparr_fill = GRU_fill(nparr_fill.copy(), tcc_model_gru_fill, tcc_model_gru_RRR_fill, show_progress=False)\n",
    "\n",
    "        return nparr_fill\n",
    "\n",
    "    ## Bench Mark Models\n",
    "\n",
    "    ### Model Definition\n",
    "\n",
    "    #### GRU\n",
    "\n",
    "    class MLTCC_Model_GRU_Normalized_TCC:\n",
    "        def __init__(self):\n",
    "            self.NMR = utl.Normalizer()\n",
    "\n",
    "            self.input_size = 8\n",
    "            self.output_size = 1\n",
    "            self.batch_size = 10000\n",
    "            self.step_size = 1e-3\n",
    "\n",
    "            h_gru = 50\n",
    "            h1 = 20\n",
    "            h2 = 40\n",
    "            h3 = 20\n",
    "\n",
    "            self.gru = nn.GRU(self.input_size, h_gru, 2, batch_first=False)   # (seq, batch, feature)\n",
    "            self.fc1 = nn.Linear(h_gru, h1)\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.fc3 = nn.Linear(h2, h3)\n",
    "            self.fc4 = nn.Linear(h3, self.output_size)\n",
    "\n",
    "            variable_list = [\n",
    "                    self.fc1.weight, self.fc1.bias,\n",
    "                    self.fc2.weight, self.fc2.bias,\n",
    "                    self.fc3.weight, self.fc3.bias,\n",
    "                    self.fc4.weight, self.fc4.bias\n",
    "                ]\n",
    "            for all_weights in self.gru.all_weights:\n",
    "                for weights in all_weights:\n",
    "                    variable_list.append(weights)\n",
    "\n",
    "\n",
    "            self.optim = torch.optim.Adam(\n",
    "                variable_list,\n",
    "                lr=self.step_size\n",
    "            )\n",
    "\n",
    "        def foward(self, X):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "    #         seq_X_result = []\n",
    "    #         for seq_X in X:\n",
    "    #             seq_X_result.append(torch.relu(self.fc1(seq_X.float())))\n",
    "    #         seq_X_result = tsr(seq_X_result)\n",
    "            # normalize\n",
    "            newX = []\n",
    "            for idx in np.arange(X.shape[0]):\n",
    "                newX.append(self.NMR.normalize_input(X[idx]).detach().numpy())\n",
    "            X = torch.tensor(np.array(newX))\n",
    "\n",
    "            X = X.float()\n",
    "\n",
    "            X_gru_outputs, hn = self.gru(X)\n",
    "            X = torch.relu(X_gru_outputs[-1]) # use last output\n",
    "\n",
    "            X = torch.relu(self.fc1(X))\n",
    "            X = torch.relu(self.fc2(X))\n",
    "            X = torch.relu(self.fc3(X))\n",
    "            X = self.fc4(X)\n",
    "\n",
    "            return self.NMR.denormalize_output(X)\n",
    "\n",
    "        def loss(self, y, y_hat):\n",
    "            \"\"\"\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "            return F.mse_loss(y.float(), y_hat.float())\n",
    "\n",
    "        def train(self, X, y_hat, max_epoch):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "            sample_count = X.shape[1]\n",
    "\n",
    "            epoch_num = []\n",
    "            losses = []\n",
    "\n",
    "            for i in np.arange(max_epoch):\n",
    "                for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                    X_batch = torch.tensor(X[:, idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_batch = self.foward(X_batch)\n",
    "                    loss = self.loss(y_batch.t(), y_hat_batch.t())\n",
    "\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "                X_batch = torch.tensor(X[:, idx_rand, :]).float()\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_rand, :]).float()\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                epoch_num.append(i)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            return (epoch_num, losses)\n",
    "\n",
    "\n",
    "        def train_time_series(self, X, y_hat, max_epoch, max_epoch_per_time):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "            progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "\n",
    "            # setting normalizer\n",
    "            # flatten X and y_hat\n",
    "            X_flat = np.concatenate(X, axis=0)\n",
    "            y_hat_flat = np.concatenate(y_hat, axis=0)\n",
    "\n",
    "            self.NMR.set_mean_and_sd(torch.tensor(X_flat), torch.tensor(y_hat_flat))\n",
    "\n",
    "\n",
    "            full_losses = np.array([])\n",
    "\n",
    "            T_running = utl.get_runtime_marker()\n",
    "            for i in np.arange(max_epoch):\n",
    "                for time in np.arange(9):\n",
    "                    (x_axis, losses) = self.train(X[:time+1, :, :], y_hat[time, :, :], max_epoch_per_time)\n",
    "                    full_losses = np.concatenate((full_losses, losses))\n",
    "                progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "                \n",
    "            progress_bar.update().end()\n",
    "\n",
    "            return (np.arange(len(full_losses)) + 1, full_losses)\n",
    "\n",
    "    #### FC\n",
    "\n",
    "    class MLTCC_Model_FC_Normalized_TCC:\n",
    "        def __init__(self):\n",
    "            self.NMR = utl.Normalizer()\n",
    "\n",
    "            self.input_size = 8\n",
    "            self.output_size = 1\n",
    "            self.batch_size = 10000\n",
    "            self.step_size = 1e-3\n",
    "\n",
    "            h1 = 20\n",
    "            h2 = 40\n",
    "            h3 = 20\n",
    "\n",
    "            W0_rand = 1/np.sqrt(self.input_size)\n",
    "            W0_init = np.random.uniform(-W0_rand, W0_rand, [h1, self.input_size])\n",
    "\n",
    "            b0_rand = 1/np.sqrt(1)\n",
    "            b0_init = np.random.uniform(-b0_rand, b0_rand, [h1, 1])\n",
    "\n",
    "            W1_rand = 1/np.sqrt(h1)\n",
    "            W1_init = np.random.uniform(-W1_rand, W1_rand, [h2, h1])\n",
    "\n",
    "            b1_rand = 1/np.sqrt(1)\n",
    "            b1_init = np.random.uniform(-b1_rand, b1_rand, [h2, 1])\n",
    "\n",
    "            W2_rand = 1/np.sqrt(h2)\n",
    "            W2_init = np.random.uniform(-W1_rand, W1_rand, [h3, h2])\n",
    "\n",
    "            b2_rand = 1/np.sqrt(1)\n",
    "            b2_init = np.random.uniform(-b1_rand, b1_rand, [h3, 1])\n",
    "\n",
    "            W3_rand = 1/np.sqrt(h3)\n",
    "            W3_init = np.random.uniform(-W1_rand, W1_rand, [self.output_size, h3])\n",
    "\n",
    "            b3_rand = 1/np.sqrt(1)\n",
    "            b3_init = np.random.uniform(-b1_rand, b1_rand, [self.output_size, 1])\n",
    "\n",
    "            self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "            self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "            self.W1 = torch.tensor(W1_init, requires_grad=True)\n",
    "            self.b1 = torch.tensor(b1_init, requires_grad=True)\n",
    "            self.W2 = torch.tensor(W2_init, requires_grad=True)\n",
    "            self.b2 = torch.tensor(b2_init, requires_grad=True)\n",
    "            self.W3 = torch.tensor(W3_init, requires_grad=True)\n",
    "            self.b3 = torch.tensor(b3_init, requires_grad=True)\n",
    "    #         self.fc1 = nn.Linear(self.input_size, h1)\n",
    "    #         self.fc2 = nn.Linear(h1, h2)\n",
    "    #         self.fc3 = nn.Linear(h2, self.output_size)\n",
    "\n",
    "            self.optim = torch.optim.Adam(\n",
    "                [\n",
    "                    self.W0, self.b0,\n",
    "                    self.W1, self.b1,\n",
    "                    self.W2, self.b2,\n",
    "                    self.W3, self.b3\n",
    "                ],\n",
    "    #             [\n",
    "    #                 self.fc1.weight, self.fc1.bias,\n",
    "    #                 self.fc2.weight, self.fc2.bias,\n",
    "    #                 self.fc3.weight, self.fc3.bias,\n",
    "    #             ],\n",
    "                lr=self.step_size\n",
    "            )\n",
    "\n",
    "        def foward(self, X):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "            # W1 * (relu(W0 * X + b0)) + b1\n",
    "\n",
    "    #         X_ = X.clone().type(torch.FloatTensor)\n",
    "\n",
    "    #         X_ = F.relu(self.fc1(X_))\n",
    "    #         X_ = F.relu(self.fc2(X_))\n",
    "    #         X_ = self.fc3(X_)\n",
    "\n",
    "    #         return X_\n",
    "    #         X = torch.tanh(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "\n",
    "            X = self.NMR.normalize_input(X)\n",
    "\n",
    "            X = torch.relu(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "            X = torch.relu(torch.matmul(self.W1, X) + self.b1)\n",
    "            X = torch.relu(torch.matmul(self.W2, X) + self.b2)\n",
    "            X = torch.matmul(self.W3, X) + self.b3\n",
    "            return self.NMR.denormalize_output(X.t())\n",
    "\n",
    "        def loss(self, y, y_hat):\n",
    "            \"\"\"\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "            return F.mse_loss(y, y_hat)\n",
    "\n",
    "        def train(self, X, y_hat, max_epoch):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "            progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "\n",
    "            self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "\n",
    "            sample_count = X.shape[0]\n",
    "\n",
    "            epoch_num = []\n",
    "            losses = []\n",
    "\n",
    "            for i in np.arange(max_epoch):\n",
    "                for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                    X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_batch = self.foward(X_batch)\n",
    "                    loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "                X_batch = torch.tensor(X[idx_rand, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                epoch_num.append(i)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "                \n",
    "            progress_bar.update().end()\n",
    "\n",
    "            return (epoch_num, losses)\n",
    "\n",
    "    #### Linear\n",
    "\n",
    "    class MLTCC_Model_Linear_Normalized_TCC:\n",
    "        def __init__(self):\n",
    "            self.NMR = utl.Normalizer()\n",
    "\n",
    "            self.input_size = 8\n",
    "            self.output_size = 1\n",
    "            self.batch_size = 50000\n",
    "            self.step_size = 1e-3\n",
    "\n",
    "            W0_rand = 1/np.sqrt(self.input_size)\n",
    "            W0_init = np.random.uniform(-W0_rand, W0_rand, [self.output_size, self.input_size])\n",
    "\n",
    "            b0_rand = 1/np.sqrt(1)\n",
    "            b0_init = np.random.uniform(-b0_rand, b0_rand, [self.output_size, 1])\n",
    "\n",
    "            self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "            self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "\n",
    "            self.optim = torch.optim.Adam(\n",
    "                [\n",
    "                    self.W0, self.b0\n",
    "                ],\n",
    "                lr=self.step_size\n",
    "            )\n",
    "\n",
    "        def foward(self, X):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "\n",
    "            X = self.NMR.normalize_input(X)\n",
    "\n",
    "            X = torch.matmul(self.W0, X.t()) + self.b0\n",
    "\n",
    "            return self.NMR.denormalize_output(X.t())\n",
    "\n",
    "        def loss(self, y, y_hat):\n",
    "            \"\"\"\n",
    "            y := [TCC],                  (sample_size, 1) like tensor\n",
    "            y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "            \"\"\"\n",
    "            return F.mse_loss(y, y_hat)\n",
    "\n",
    "        def train(self, X, y_hat, max_epoch):\n",
    "            \"\"\"\n",
    "            X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "            y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "            \"\"\"\n",
    "            progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "\n",
    "            self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "\n",
    "            sample_count = X.shape[0]\n",
    "\n",
    "            epoch_num = []\n",
    "            losses = []\n",
    "\n",
    "            for i in np.arange(max_epoch):\n",
    "                for idx_batch in np.arange(int(sample_count / self.batch_size))* self.batch_size:\n",
    "                    X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                    y_batch = self.foward(X_batch)\n",
    "                    loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "                X_batch = torch.tensor(X[idx_rand, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "\n",
    "                epoch_num.append(i)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "                \n",
    "            progress_bar.update().end()\n",
    "\n",
    "            return (epoch_num, losses)\n",
    "\n",
    "\n",
    "    ### Bench Mark Model Training\n",
    "\n",
    "    # # GRU\n",
    "    # tcc_model_gru_TCC = MLTCC_Model_GRU_Normalized_TCC()\n",
    "\n",
    "    # (x_axis, losses) = tcc_model_gru_TCC.train_time_series(X_train, y_hat_train[:, :, -1:], 50, 2)\n",
    "\n",
    "    # # FC\n",
    "    # X_train_FC = flatten_time_axis(X_train)\n",
    "    # y_hat_train_FC = flatten_time_axis(y_hat_train[:, :, -1:])\n",
    "    # X_test_FC = flatten_time_axis(X_test)\n",
    "    # y_hat_test_FC = flatten_time_axis(y_hat_test[:, :, -1:])\n",
    "\n",
    "    # tcc_model_FC_TCC = MLTCC_Model_FC_Normalized_TCC()\n",
    "\n",
    "    # (x_axis, losses) = tcc_model_FC_TCC.train(X_train_FC, y_hat_train_FC, 2000)\n",
    "\n",
    "    # # Linear\n",
    "    # X_train_Linear = flatten_time_axis(X_train)\n",
    "    # y_hat_train_Linear = flatten_time_axis(y_hat_train[:, :, -1:])\n",
    "    # X_test_Linear = flatten_time_axis(X_test)\n",
    "    # y_hat_test_Linear = flatten_time_axis(y_hat_test[:, :, -1:])\n",
    "\n",
    "    # tcc_model_Linear_TCC = MLTCC_Model_Linear_Normalized_TCC()\n",
    "\n",
    "    # (x_axis, losses) = tcc_model_Linear_TCC.train(X_train_Linear, y_hat_train_Linear, 500)\n",
    "\n",
    "    ## MSE, MAE Bench Mark Functions; Data Masking Functions\n",
    "\n",
    "    def MSE(x_fill, x, masked):\n",
    "        \n",
    "        x_mean_arr = np.mean(x, axis=(0, 1))\n",
    "        x_std_arr = np.std(x, axis=(0, 1))\n",
    "\n",
    "        for idx in np.arange(x_std_arr.shape[0]):\n",
    "            if (np.abs(x_std_arr[idx]) < 1e-7):\n",
    "                x_std_arr[idx] = 1\n",
    "        x_norm = (x - x_mean_arr) / x_std_arr\n",
    "        x_fill_norm = (x_fill - x_mean_arr) / x_std_arr\n",
    "        \n",
    "        ret_val = []\n",
    "        \n",
    "        for idx in np.arange(x_fill.shape[2]):\n",
    "            if (np.count_nonzero(np.isnan(masked[:, :, idx])) == 0):\n",
    "                # No data are masked\n",
    "                ret_val.append(0)\n",
    "            else:\n",
    "                if USING_STANDARD_DIVIATION:\n",
    "                    ret_val.append(np.mean(np.power(x_fill_norm[:, :, idx] - x_norm[:, :, idx], 2)))\n",
    "                else:\n",
    "                    ret_val.append(np.mean(np.power(x_fill[:, :, idx] - x[:, :, idx], 2)))\n",
    "        \n",
    "        return np.array(ret_val)\n",
    "\n",
    "    def MAE(x_fill, x, masked):\n",
    "        x_mean_arr = np.mean(x, axis=(0, 1))\n",
    "        x_std_arr = np.std(x, axis=(0, 1))\n",
    "\n",
    "        for idx in np.arange(x_std_arr.shape[0]):\n",
    "            if (np.abs(x_std_arr[idx]) < 1e-7):\n",
    "                x_std_arr[idx] = 1\n",
    "        x_norm = (x - x_mean_arr) / x_std_arr\n",
    "        x_fill_norm = (x_fill - x_mean_arr) / x_std_arr\n",
    "        \n",
    "        ret_val = []\n",
    "        \n",
    "        for idx in np.arange(x_fill.shape[2]):\n",
    "            if (np.count_nonzero(np.isnan(masked[:, :, idx])) == 0):\n",
    "                # No data are masked\n",
    "                ret_val.append(0)\n",
    "            else:\n",
    "                if USING_STANDARD_DIVIATION:\n",
    "                    ret_val.append(np.mean(np.abs(x_fill_norm[:, :, idx] - x_norm[:, :, idx])))\n",
    "                else:\n",
    "                    ret_val.append(np.mean(np.abs(x_fill[:, :, idx] - x[:, :, idx])))\n",
    "        \n",
    "        return np.array(ret_val)\n",
    "\n",
    "\n",
    "    def mask_data(a, percent_missing_data):\n",
    "        a = a.copy()\n",
    "        a.ravel()[np.random.choice(a.size, int(percent_missing_data * a.size), replace=False)]= np.nan\n",
    "        return a\n",
    "\n",
    "    def mask_data_column(a, percent_missing_data):\n",
    "        \"\"\"\n",
    "        This function will perserve at least one timestamp per sample\n",
    "        \"\"\"\n",
    "        a = a.copy()\n",
    "\n",
    "        # save at least 1 column\n",
    "        preserve_time_idx = np.random.randint(a.shape[0], size=a.shape[1])\n",
    "        perserve_time_data = []\n",
    "        for i1 in np.arange(a.shape[1]):\n",
    "            perserve_time_data.append(a[preserve_time_idx[i1], i1, :])\n",
    "\n",
    "        a = mask_data(a, percent_missing_data)\n",
    "\n",
    "        for i1 in np.arange(a.shape[1]):\n",
    "            a[preserve_time_idx[i1], i1, :] = perserve_time_data[i1]\n",
    "\n",
    "        return a\n",
    "\n",
    "    def mask_data_column_realistic(a, percent_missing_data):\n",
    "        \"\"\"\n",
    "        This function will perserve at least one timestamp per sample\n",
    "        The masked data will be contiguous chuck\n",
    "        \"\"\"\n",
    "        a = a.copy()\n",
    "\n",
    "        # progress = utl.TimedProgressBar(a.shape[1]).update_msg(\"Masking Data; percent masking: {0}\".format(percent_missing_data)).update()\n",
    "\n",
    "        for i1 in np.arange(a.shape[1]):\n",
    "            wipe_data_count = np.minimum(np.count_nonzero(np.random.random(9) <= percent_missing_data), 8) # keep at least 1 column\n",
    "\n",
    "            if (wipe_data_count > 0):\n",
    "                # we need to wipe some data\n",
    "                start_idx = np.random.randint(a.shape[0] - wipe_data_count + 1) # inclusive\n",
    "                end_idx = start_idx + wipe_data_count                           # exclusive\n",
    "                a[start_idx:end_idx, i1, :] = np.nan\n",
    "\n",
    "            # progress.inc_val().update()\n",
    "\n",
    "        # progress.update().end()\n",
    "\n",
    "        return a\n",
    "\n",
    "    ## Random Masking\n",
    "\n",
    "    percent_list = np.arange(0, 1, 0.1) # from 0 to 0.9 with an increment of 0.1\n",
    "    fill_MSE = {}\n",
    "    fill_MAE = {}\n",
    "\n",
    "    X_test_filled_overall = {}\n",
    "\n",
    "    fill_MSE[\"PercentMissing\"] = percent_list\n",
    "    fill_MAE[\"PercentMissing\"] = percent_list\n",
    "\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Generate Data\").update()\n",
    "    masked_data = []\n",
    "    X_test_filled_overall[\"Masked\"] = []\n",
    "    for percent_missing in percent_list:\n",
    "        masked_data.append(mask_data_column(X_test, percent_missing)) \n",
    "        X_test_filled_overall[\"Masked\"].append(masked_data[-1])\n",
    "        progress_bar.inc_val().update()\n",
    "    print()\n",
    "\n",
    "    fill_MSE[\"GRU\"] = []\n",
    "    fill_MAE[\"GRU\"] = []\n",
    "    X_test_filled_overall[\"GRU\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"GRU Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = GRU_fill(masked_data_sample.copy(), tcc_model_gru, tcc_model_gru_RRR, show_progress=False)\n",
    "        X_test_filled_overall[\"GRU\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"GRU\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"GRU\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "\n",
    "    fill_MSE[\"Mean\"] = []\n",
    "    fill_MAE[\"Mean\"] = []\n",
    "    X_test_filled_overall[\"Mean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"Mean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Mean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Mean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "\n",
    "    fill_MSE[\"DailyMean\"] = []\n",
    "    fill_MAE[\"DailyMean\"] = []\n",
    "    X_test_filled_overall[\"DailyMean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Daily Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = daily_mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DailyMean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DailyMean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DailyMean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"DayNightMean\"] = []\n",
    "    fill_MAE[\"DayNightMean\"] = []\n",
    "    X_test_filled_overall[\"DayNightMean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Day Night Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = day_night_mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DayNightMean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DayNightMean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DayNightMean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"Linear\"] = []\n",
    "    fill_MAE[\"Linear\"] = []\n",
    "    X_test_filled_overall[\"Linear\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Linear Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = linear_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"Linear\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Linear\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Linear\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"DayNightLinear\"] = []\n",
    "    fill_MAE[\"DayNightLinear\"] = []\n",
    "    X_test_filled_overall[\"DayNightLinear\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Day Night Linear Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = day_night_linear_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DayNightLinear\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DayNightLinear\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DayNightLinear\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    # fill_MSE[\"Cubic\"] = []\n",
    "    # fill_MAE[\"Cubic\"] = []\n",
    "    # X_test_filled_overall[\"Cubic\"] = []\n",
    "    # progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Cubic Spline Fill\").update()\n",
    "    # for masked_data_sample in masked_data:\n",
    "    #     X_test_filled = cubic_spline_fill(masked_data_sample.copy())\n",
    "    #     X_test_filled_overall[\"Cubic\"].append(X_test_filled)\n",
    "\n",
    "    #     fill_MSE[\"Cubic\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "    #     fill_MAE[\"Cubic\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "    #     progress_bar.inc_val().update()\n",
    "\n",
    "    # progress_bar.update().end()\n",
    "\n",
    "    # fill_MSE[\"HybridMux\"] = []\n",
    "    # fill_MAE[\"HybridMux\"] = []\n",
    "    # X_test_filled_overall[\"HybridMux\"] = []\n",
    "    # progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Hybrid Mux Fill\").update()\n",
    "    # for masked_data_sample in masked_data:\n",
    "    #     X_test_filled = GRU_linear_hybrid_mux_fill(masked_data_sample.copy(), 4, tcc_model_gru, tcc_model_gru_RRR)\n",
    "    #     X_test_filled_overall[\"HybridMux\"].append(X_test_filled)\n",
    "\n",
    "    #     fill_MSE[\"HybridMux\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "    #     fill_MAE[\"HybridMux\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "    #     progress_bar.inc_val().update()\n",
    "\n",
    "    # progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"Hybrid\"] = []\n",
    "    fill_MAE[\"Hybrid\"] = []\n",
    "    X_test_filled_overall[\"Hybrid\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Hybrid Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = GRU_linear_hybrid_fill(masked_data_sample.copy(), 4, tcc_model_gru, tcc_model_gru_RRR)\n",
    "        X_test_filled_overall[\"Hybrid\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Hybrid\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Hybrid\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    save_all_data(RANDOM_DATA_FOLDER_ARCHIVE, utl.get_current_day_time_string(), fill_MSE, fill_MAE)\n",
    "\n",
    "\n",
    "\n",
    "    ## Realistic Masking\n",
    "    # The data will be mask in contiguous chuck\n",
    "\n",
    "    percent_list = np.arange(0, 1, 0.1) # from 0 to 0.9 with an increment of 0.1\n",
    "    fill_MSE = {}\n",
    "    fill_MAE = {}\n",
    "    X_test_filled_overall = {}\n",
    "\n",
    "    fill_MSE[\"PercentMissing\"] = percent_list\n",
    "    fill_MAE[\"PercentMissing\"] = percent_list\n",
    "\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Generate Data\").update()\n",
    "    masked_data = []\n",
    "    X_test_filled_overall[\"Masked\"] = []\n",
    "    for percent_missing in percent_list:\n",
    "        masked_data.append(mask_data_column_realistic(X_test, percent_missing)) \n",
    "        X_test_filled_overall[\"Masked\"].append(masked_data[-1])\n",
    "        progress_bar.inc_val().update()\n",
    "    print()\n",
    "\n",
    "    fill_MSE[\"GRU\"] = []\n",
    "    fill_MAE[\"GRU\"] = []\n",
    "    X_test_filled_overall[\"GRU\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"GRU Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = GRU_fill(masked_data_sample.copy(), tcc_model_gru, tcc_model_gru_RRR, show_progress=False)\n",
    "        X_test_filled_overall[\"GRU\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"GRU\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"GRU\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"Mean\"] = []\n",
    "    fill_MAE[\"Mean\"] = []\n",
    "    X_test_filled_overall[\"Mean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"Mean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Mean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Mean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"DailyMean\"] = []\n",
    "    fill_MAE[\"DailyMean\"] = []\n",
    "    X_test_filled_overall[\"DailyMean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Daily Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = daily_mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DailyMean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DailyMean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DailyMean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"DayNightMean\"] = []\n",
    "    fill_MAE[\"DayNightMean\"] = []\n",
    "    X_test_filled_overall[\"DayNightMean\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Day Night Mean Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = day_night_mean_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DayNightMean\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DayNightMean\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DayNightMean\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"Linear\"] = []\n",
    "    fill_MAE[\"Linear\"] = []\n",
    "    X_test_filled_overall[\"Linear\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Linear Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = linear_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"Linear\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Linear\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Linear\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"DayNightLinear\"] = []\n",
    "    fill_MAE[\"DayNightLinear\"] = []\n",
    "    X_test_filled_overall[\"DayNightLinear\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Day Night Linear Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = day_night_linear_fill(masked_data_sample.copy())\n",
    "        X_test_filled_overall[\"DayNightLinear\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"DayNightLinear\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"DayNightLinear\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    # fill_MSE[\"Cubic\"] = []\n",
    "    # fill_MAE[\"Cubic\"] = []\n",
    "    # X_test_filled_overall[\"Cubic\"] = []\n",
    "    # progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Cubic Spline Fill\").update()\n",
    "    # for masked_data_sample in masked_data:\n",
    "    #     X_test_filled = cubic_spline_fill(masked_data_sample.copy())\n",
    "    #     X_test_filled_overall[\"Cubic\"].append(X_test_filled)\n",
    "\n",
    "    #     fill_MSE[\"Cubic\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "    #     fill_MAE[\"Cubic\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "    #     progress_bar.inc_val().update()\n",
    "\n",
    "    # progress_bar.update().end()\n",
    "\n",
    "\n",
    "    # fill_MSE[\"HybridMux\"] = []\n",
    "    # fill_MAE[\"HybridMux\"] = []\n",
    "    # X_test_filled_overall[\"HybridMux\"] = []\n",
    "    # progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Hybrid Mux Fill\").update()\n",
    "    # for masked_data_sample in masked_data:\n",
    "    #     X_test_filled = GRU_linear_hybrid_mux_fill(masked_data_sample.copy(), 4, tcc_model_gru, tcc_model_gru_RRR)\n",
    "    #     X_test_filled_overall[\"HybridMux\"].append(X_test_filled)\n",
    "\n",
    "    #     fill_MSE[\"HybridMux\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "    #     fill_MAE[\"HybridMux\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "    #     progress_bar.inc_val().update()\n",
    "\n",
    "    # progress_bar.update().end()\n",
    "\n",
    "    fill_MSE[\"Hybrid\"] = []\n",
    "    fill_MAE[\"Hybrid\"] = []\n",
    "    X_test_filled_overall[\"Hybrid\"] = []\n",
    "    progress_bar = utl.TimedProgressBar(percent_list.shape[0]).update_msg(\"Hybrid Fill\").update()\n",
    "    for masked_data_sample in masked_data:\n",
    "        X_test_filled = GRU_linear_hybrid_fill(masked_data_sample.copy(), 4, tcc_model_gru, tcc_model_gru_RRR)\n",
    "        X_test_filled_overall[\"Hybrid\"].append(X_test_filled)\n",
    "\n",
    "        fill_MSE[\"Hybrid\"].append(MSE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "        fill_MAE[\"Hybrid\"].append(MAE(X_test_filled, X_test, masked_data_sample.copy()))\n",
    "\n",
    "        progress_bar.inc_val().update()\n",
    "\n",
    "    progress_bar.update().end()\n",
    "\n",
    "    save_all_data(REALISTIC_DATA_OUTPUT_FOLDER_PATH_ARCHIVE, utl.get_current_day_time_string(), fill_MSE, fill_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c29498-b18b-4698-8d03-ea02d26dddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(100):\n",
    "    T = utl.get_runtime_marker()\n",
    "    work()\n",
    "    print(\"{0}; {1}\".format(i + 1, utl.format_time_s_2_hms(utl.get_runtime_in_second(T))))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
