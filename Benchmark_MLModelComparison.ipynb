{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2874f4d-3c07-4c34-8bb1-75b1b835858f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combined Benchmark with Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1a808-3dc1-4a83-a9e9-ec81a0aa9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sp_stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import LZH_Utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310524e-c138-475e-a74c-84046828cd42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9c56d-4498-4a23-bfc0-d5d499480f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"full_rank_dataset_anormaly\"\n",
    "OUTPUT_FOLDER_PATH = \"Output/combined_benchmark_anomaly/\"\n",
    "OUTPUT_FILE_PREFIX = \"y_hat_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2871bd-3231-434c-8340-888f7824f901",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b9271-6dea-455e-b8ae-7760ccecdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(file_name, data):\n",
    "    \"\"\"\n",
    "    data as {\n",
    "        \"y_hat\":     nparray,\n",
    "        \"y_hat_ERA5\":nparray,\n",
    "        \"GRU\":       nparray,\n",
    "        \"GRU_FB\":    nparray,\n",
    "        \"FC\":        nparray,\n",
    "        \"FC_FB\":     nparray,\n",
    "        \"Linear\":    nparray,\n",
    "        \"Linear_FB\": nparray,\n",
    "        \"ERA5\":      nparray,\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        np.save(f, data[\"y_hat\"])\n",
    "        np.save(f, data[\"y_hat_ERA5\"])\n",
    "        np.save(f, data[\"GRU\"])\n",
    "        np.save(f, data[\"GRU_FB\"])\n",
    "        np.save(f, data[\"FC\"])\n",
    "        np.save(f, data[\"FC_FB\"])\n",
    "        np.save(f, data[\"Linear\"])\n",
    "        np.save(f, data[\"Linear_FB\"])\n",
    "        np.save(f, data[\"ERA5\"])\n",
    "        \n",
    "    return file_name\n",
    "        \n",
    "def load_result(file_name):\n",
    "    \"\"\"\n",
    "    return data as {\n",
    "        \"y_hat\":     nparray,\n",
    "        \"y_hat_ERA5\":nparray,\n",
    "        \"GRU\":       nparray,\n",
    "        \"GRU_FB\":    nparray,\n",
    "        \"FC\":        nparray,\n",
    "        \"FC_FB\":     nparray,\n",
    "        \"Linear\":    nparray,\n",
    "        \"Linear_FB\": nparray,\n",
    "        \"ERA5\":      nparray,\n",
    "    }\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data[\"y_hat\"] = np.load(f)\n",
    "        data[\"y_hat_ERA5\"] = np.load(f)\n",
    "        data[\"GRU\"] = np.load(f)\n",
    "        data[\"GRU_FB\"] = np.load(f)\n",
    "        data[\"FC\"] = np.load(f)\n",
    "        data[\"FC_FB\"] = np.load(f)\n",
    "        data[\"Linear\"] = np.load(f)\n",
    "        data[\"Linear_FB\"] = np.load(f)\n",
    "        data[\"ERA5\"] = np.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24091325-9280-452f-b5c5-107bdf35d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_stack(arr):\n",
    "    return np.array([arr]).T\n",
    "\n",
    "def tsr(arr):\n",
    "    return torch.tensor(arr)\n",
    "\n",
    "def plot(x, y, x_label=\"\", y_label=\"\", legend=\"\", title=\"\"):\n",
    "    # plt.figure(figsize=[8, 6], dpi=300)\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    \n",
    "    if (type(legend) is list):\n",
    "        for yy in y:\n",
    "            plt.plot(x, yy)\n",
    "        plt.legend(legend)\n",
    "    else: \n",
    "        plt.plot(x, y)\n",
    "        \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d31150-443c-49fb-bc1e-b997f975d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r_value(x, y):\n",
    "    return sp_stats.linregress(x, y).rvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef593e-d3ae-47f6-ad94-2f3df8d6d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_percentage(x, y, x_label=\"\", y_label=\"\", title=\"\"):\n",
    "    return # do not output\n",
    "\n",
    "    # plt.figure(figsize=[8, 6], dpi=300)\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    \n",
    "    x_fit = np.linspace(0, 100, 1000)\n",
    "    y_fit1 = np.polyval(np.polyfit(x, y, 1), x_fit)\n",
    "    \n",
    "    plt.scatter(x, y, s=0.5, c='k')\n",
    "    plt.plot(x_fit, y_fit1, \"r\")\n",
    "        \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend([\"linear regression\", \"data\"])\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlim([0, 100])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    result = sp_stats.linregress(x, y)\n",
    "    print(\"     slope: {0}\".format(result.slope))\n",
    "    print(\" intercept: {0}\".format(result.intercept))\n",
    "    print(\"corr coeff: {0}\".format(result.rvalue))\n",
    "    print(\"  variance: {0}\".format(result.rvalue ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84cee5-ae5e-4ad2-af27-26f665c0ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_percentage_bined_average(x, y, x_label=\"\", y_label=\"\", title=\"\", bin_count=10):\n",
    "    return # do not output\n",
    "\n",
    "    # plt.figure(figsize=[8, 6], dpi=300)\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    \n",
    "    # Overall\n",
    "    x_fit = np.linspace(0, 100, 10)\n",
    "    y_fit1 = np.polyval(np.polyfit(x, y, 1), x_fit)\n",
    "    \n",
    "    plt.scatter(x, y, s=0.5, alpha=0.3, c='k', label=\"data\")\n",
    "    plt.plot(x_fit, y_fit1, \"r\", label=\"linear regression\")\n",
    "        \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    result = sp_stats.linregress(x, y)\n",
    "    print(\"Overall:\")\n",
    "    print(\"     slope: {0}\".format(result.slope))\n",
    "    print(\" intercept: {0}\".format(result.intercept))\n",
    "    print(\"corr coeff: {0}\".format(result.rvalue))\n",
    "    print(\"  variance: {0}\".format(result.rvalue ** 2))\n",
    "    print()\n",
    "    \n",
    "    # Binned Average\n",
    "    is_label_printed = False\n",
    "    \n",
    "    bin_arr = np.linspace(0, 100, bin_count+1)\n",
    "    for (low, high) in zip(bin_arr[:-1], bin_arr[1:]):\n",
    "        \n",
    "        bin_filter = np.logical_and(low <= x, x <= high)\n",
    "        x_bin = x[bin_filter]\n",
    "        y_bin = y[bin_filter]\n",
    "        \n",
    "        x_fit = np.linspace(low, high, 10)\n",
    "        y_fit1 = np.polyval(np.polyfit(x_bin, y_bin, 1), x_fit)\n",
    "        \n",
    "        if (not is_label_printed):\n",
    "            plt.plot(x_fit, y_fit1, \"b\", label=\"binned linear reg.\")\n",
    "            is_label_printed = True\n",
    "        else:\n",
    "            plt.plot(x_fit, y_fit1, \"b\")\n",
    "            \n",
    "        result = sp_stats.linregress(x_bin, y_bin)\n",
    "        print(\"[{4:>3}, {5:>3}):   b:{0:.4f};  m:{1:.4f}; r:{2:.4f}; var:{3:.4f}\".format(result.slope, result.intercept, result.rvalue, result.rvalue ** 2, int(low), int(high)))\n",
    "        \n",
    "\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlim([0, 100])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba8785-b40b-4c8e-a8b4-4904f34c9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_and_testing_set():\n",
    "    \"\"\"\n",
    "    return format: (idx_training_set, idx_test_set)\n",
    "    \"\"\"\n",
    "    df = utl.read_time_series_data(DATA_FILE_PATH)\n",
    "    idx_test_set = np.random.choice(np.arange(df[0].shape[0]), [int(0.1 * df[0].shape[0])], False)\n",
    "    idx_training_set = np.delete(np.arange(df[0].shape[0]), idx_test_set)\n",
    "    \n",
    "    return (idx_training_set, idx_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fb516-15be-411b-8d0f-2cd100790afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(idx_test_set, idx_training_set):\n",
    "    \"\"\"\n",
    "    return format: ((X_train, y_hat_train), (X_test, y_hat_test))\n",
    "    \"\"\"\n",
    "    df = utl.read_time_series_data(DATA_FILE_PATH)\n",
    "\n",
    "    df[0][\"TCC_FB\"] = 0\n",
    "    for idx in np.arange(8):\n",
    "        df[idx+1][\"TCC_FB\"] = df[idx][\"TCC\"]\n",
    "    \n",
    "    # assigning new order\n",
    "    new_col = ['LTS', 'SST', 'Subsidence', 'Night_Day', 'RH', 'q', 'wsp',\n",
    "               'LTS_A', 'SST_A', 'Subsidence_A', 'RH_A', 'q_A', 'wsp_A', 'TCC']\n",
    "\n",
    "    for idx in np.arange(len(df)):\n",
    "        df[idx] = df[idx][new_col]\n",
    "    \n",
    "    time_arr = np.arange(9)\n",
    "\n",
    "    X_full = [df[time].iloc[:, :-1].to_numpy() for time in time_arr]\n",
    "    y_hat_full = [np.c_[df[time].iloc[:, -1].to_numpy()] for time in time_arr]\n",
    "\n",
    "    X_train = np.array([X_full[time][idx_training_set] for time in time_arr])\n",
    "    y_hat_train = np.array([y_hat_full[time][idx_training_set] for time in time_arr])\n",
    "\n",
    "    X_test = np.array([X_full[time][idx_test_set] for time in time_arr])\n",
    "    y_hat_test = np.array([y_hat_full[time][idx_test_set] for time in time_arr])\n",
    "    return ((X_train, y_hat_train), (X_test, y_hat_test))\n",
    "\n",
    "def get_data_TCC_feedback(idx_test_set, idx_training_set):\n",
    "    \"\"\"\n",
    "    return format: ((X_train, y_hat_train), (X_test, y_hat_test))\n",
    "    \"\"\"\n",
    "    df = utl.read_time_series_data(DATA_FILE_PATH)\n",
    "    df[0][\"TCC_FB\"] = 0\n",
    "    for idx in np.arange(8):\n",
    "        df[idx+1][\"TCC_FB\"] = df[idx][\"TCC\"]\n",
    "    \n",
    "    # assigning new order\n",
    "    new_col = ['LTS', 'SST', 'Subsidence', 'Night_Day', 'RH', 'q', 'wsp',\n",
    "               'LTS_A', 'SST_A', 'Subsidence_A', 'RH_A', 'q_A', 'wsp_A', 'TCC_FB', 'TCC']\n",
    "\n",
    "    for idx in np.arange(len(df)):\n",
    "        df[idx] = df[idx][new_col]\n",
    "    \n",
    "    time_arr = np.arange(9)\n",
    "\n",
    "    X_full = [df[time].iloc[:, :-1].to_numpy() for time in time_arr]\n",
    "    y_hat_full = [np.c_[df[time].iloc[:, -1].to_numpy()] for time in time_arr]\n",
    "\n",
    "    X_train = np.array([X_full[time][idx_training_set] for time in time_arr])\n",
    "    y_hat_train = np.array([y_hat_full[time][idx_training_set] for time in time_arr])\n",
    "\n",
    "    X_test = np.array([X_full[time][idx_test_set] for time in time_arr])\n",
    "    y_hat_test = np.array([y_hat_full[time][idx_test_set] for time in time_arr])\n",
    "    \n",
    "    return ((X_train, y_hat_train), (X_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b8aa6-185c-4250-b4f2-7a1fbf274092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_time_axis(data):\n",
    "    \"\"\"\n",
    "    flatten the time axis so it could be used in FC and linear model. \n",
    "    \"\"\"\n",
    "    return np.concatenate(data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a9bc1-584d-4de2-9746-34f4dd5b7c46",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563391b-64d7-4485-aed5-3768e122aef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2098094-3166-473f-b998-67779a4c1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_GRU_Normalized:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 13\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 10000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        h_gru = 50\n",
    "        h1 = 20\n",
    "        h2 = 40\n",
    "        h3 = 20\n",
    "        \n",
    "        self.gru = nn.GRU(self.input_size, h_gru, 2, batch_first=False)   # (seq, batch, feature)\n",
    "        self.fc1 = nn.Linear(h_gru, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.fc4 = nn.Linear(h3, self.output_size)\n",
    "        \n",
    "        variable_list = [\n",
    "                self.fc1.weight, self.fc1.bias,\n",
    "                self.fc2.weight, self.fc2.bias,\n",
    "                self.fc3.weight, self.fc3.bias,\n",
    "                self.fc4.weight, self.fc4.bias\n",
    "            ]\n",
    "        for all_weights in self.gru.all_weights:\n",
    "            for weights in all_weights:\n",
    "                variable_list.append(weights)\n",
    "        \n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            variable_list,\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "#         seq_X_result = []\n",
    "#         for seq_X in X:\n",
    "#             seq_X_result.append(torch.relu(self.fc1(seq_X.float())))\n",
    "#         seq_X_result = tsr(seq_X_result)\n",
    "        # normalize\n",
    "        newX = []\n",
    "        for idx in np.arange(X.shape[0]):\n",
    "            newX.append(self.NMR.normalize_input(X[idx]).detach().numpy())\n",
    "        X = torch.tensor(np.array(newX))\n",
    "\n",
    "        X = X.float()\n",
    "    \n",
    "        X_gru_outputs, hn = self.gru(X)\n",
    "        X = torch.relu(X_gru_outputs[-1]) # use last output\n",
    "        \n",
    "        X = torch.relu(self.fc1(X))\n",
    "        X = torch.relu(self.fc2(X))\n",
    "        X = torch.relu(self.fc3(X))\n",
    "        X = self.fc4(X)\n",
    "        \n",
    "        return self.NMR.denormalize_output(X)\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y.float(), y_hat.float())\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        sample_count = X.shape[1]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                X_batch = torch.tensor(X[:, idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch.t(), y_hat_batch.t())\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[:, idx_rand, :]).float()\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :]).float()\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        return (epoch_num, losses)\n",
    "    \n",
    "    \n",
    "    def train_time_series(self, X, y_hat, max_epoch, max_epoch_per_time):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        # setting normalizer\n",
    "        # flatten X and y_hat\n",
    "        X_flat = np.concatenate(X, axis=0)\n",
    "        y_hat_flat = np.concatenate(y_hat, axis=0)\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X_flat), torch.tensor(y_hat_flat))\n",
    "        \n",
    "        \n",
    "        full_losses = np.array([])\n",
    "\n",
    "        T_running = utl.get_runtime_marker()\n",
    "        for i in np.arange(max_epoch):\n",
    "            for time in np.arange(9):\n",
    "                (x_axis, losses) = self.train(X[:time+1, :, :], y_hat[time, :, :], max_epoch_per_time)\n",
    "                full_losses = np.concatenate((full_losses, losses))\n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "            \n",
    "        return (np.arange(len(full_losses)) + 1, full_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dba708-6e5e-41e0-8071-55d51fb99845",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GRU with TCC Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b699a33-be31-430e-9244-b9ec9cd3137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_GRU_Normalized_TCC:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 14\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 10000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        h_gru = 50\n",
    "        h1 = 20\n",
    "        h2 = 40\n",
    "        h3 = 20\n",
    "        \n",
    "        self.gru = nn.GRU(self.input_size, h_gru, 2, batch_first=False)   # (seq, batch, feature)\n",
    "        self.fc1 = nn.Linear(h_gru, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.fc4 = nn.Linear(h3, self.output_size)\n",
    "        \n",
    "        variable_list = [\n",
    "                self.fc1.weight, self.fc1.bias,\n",
    "                self.fc2.weight, self.fc2.bias,\n",
    "                self.fc3.weight, self.fc3.bias,\n",
    "                self.fc4.weight, self.fc4.bias\n",
    "            ]\n",
    "        for all_weights in self.gru.all_weights:\n",
    "            for weights in all_weights:\n",
    "                variable_list.append(weights)\n",
    "        \n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            variable_list,\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "#         seq_X_result = []\n",
    "#         for seq_X in X:\n",
    "#             seq_X_result.append(torch.relu(self.fc1(seq_X.float())))\n",
    "#         seq_X_result = tsr(seq_X_result)\n",
    "        # normalize\n",
    "        newX = []\n",
    "        for idx in np.arange(X.shape[0]):\n",
    "            newX.append(self.NMR.normalize_input(X[idx]).detach().numpy())\n",
    "        X = torch.tensor(np.array(newX))\n",
    "\n",
    "        X = X.float()\n",
    "    \n",
    "        X_gru_outputs, hn = self.gru(X)\n",
    "        X = torch.relu(X_gru_outputs[-1]) # use last output\n",
    "        \n",
    "        X = torch.relu(self.fc1(X))\n",
    "        X = torch.relu(self.fc2(X))\n",
    "        X = torch.relu(self.fc3(X))\n",
    "        X = self.fc4(X)\n",
    "        \n",
    "        return self.NMR.denormalize_output(X)\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y.float(), y_hat.float())\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        sample_count = X.shape[1]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                X_batch = torch.tensor(X[:, idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch.t(), y_hat_batch.t())\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[:, idx_rand, :]).float()\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :]).float()\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        return (epoch_num, losses)\n",
    "    \n",
    "    \n",
    "    def train_time_series(self, X, y_hat, max_epoch, max_epoch_per_time):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        # setting normalizer\n",
    "        # flatten X and y_hat\n",
    "        X_flat = np.concatenate(X, axis=0)\n",
    "        y_hat_flat = np.concatenate(y_hat, axis=0)\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X_flat), torch.tensor(y_hat_flat))\n",
    "        \n",
    "        \n",
    "        full_losses = np.array([])\n",
    "\n",
    "        T_running = utl.get_runtime_marker()\n",
    "        for i in np.arange(max_epoch):\n",
    "            for time in np.arange(9):\n",
    "                (x_axis, losses) = self.train(X[:time+1, :, :], y_hat[time, :, :], max_epoch_per_time)\n",
    "                full_losses = np.concatenate((full_losses, losses))\n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "            \n",
    "        return (np.arange(len(full_losses)) + 1, full_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af9415-fdcf-48ec-948a-3ea2af3199c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf7c73-3e5e-490c-829f-7c86b8fbbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_FC_Normalized:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 13\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 10000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        h1 = 20\n",
    "        h2 = 40\n",
    "        h3 = 20\n",
    "        \n",
    "        W0_rand = 1/np.sqrt(self.input_size)\n",
    "        W0_init = np.random.uniform(-W0_rand, W0_rand, [h1, self.input_size])\n",
    "\n",
    "        b0_rand = 1/np.sqrt(1)\n",
    "        b0_init = np.random.uniform(-b0_rand, b0_rand, [h1, 1])\n",
    "\n",
    "        W1_rand = 1/np.sqrt(h1)\n",
    "        W1_init = np.random.uniform(-W1_rand, W1_rand, [h2, h1])\n",
    "\n",
    "        b1_rand = 1/np.sqrt(1)\n",
    "        b1_init = np.random.uniform(-b1_rand, b1_rand, [h2, 1])\n",
    "        \n",
    "        W2_rand = 1/np.sqrt(h2)\n",
    "        W2_init = np.random.uniform(-W1_rand, W1_rand, [h3, h2])\n",
    "\n",
    "        b2_rand = 1/np.sqrt(1)\n",
    "        b2_init = np.random.uniform(-b1_rand, b1_rand, [h3, 1])\n",
    "        \n",
    "        W3_rand = 1/np.sqrt(h3)\n",
    "        W3_init = np.random.uniform(-W1_rand, W1_rand, [self.output_size, h3])\n",
    "\n",
    "        b3_rand = 1/np.sqrt(1)\n",
    "        b3_init = np.random.uniform(-b1_rand, b1_rand, [self.output_size, 1])\n",
    "\n",
    "        self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "        self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "        self.W1 = torch.tensor(W1_init, requires_grad=True)\n",
    "        self.b1 = torch.tensor(b1_init, requires_grad=True)\n",
    "        self.W2 = torch.tensor(W2_init, requires_grad=True)\n",
    "        self.b2 = torch.tensor(b2_init, requires_grad=True)\n",
    "        self.W3 = torch.tensor(W3_init, requires_grad=True)\n",
    "        self.b3 = torch.tensor(b3_init, requires_grad=True)\n",
    "#         self.fc1 = nn.Linear(self.input_size, h1)\n",
    "#         self.fc2 = nn.Linear(h1, h2)\n",
    "#         self.fc3 = nn.Linear(h2, self.output_size)\n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            [\n",
    "                self.W0, self.b0,\n",
    "                self.W1, self.b1,\n",
    "                self.W2, self.b2,\n",
    "                self.W3, self.b3\n",
    "            ],\n",
    "#             [\n",
    "#                 self.fc1.weight, self.fc1.bias,\n",
    "#                 self.fc2.weight, self.fc2.bias,\n",
    "#                 self.fc3.weight, self.fc3.bias,\n",
    "#             ],\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        # W1 * (relu(W0 * X + b0)) + b1\n",
    "        \n",
    "#         X_ = X.clone().type(torch.FloatTensor)\n",
    "        \n",
    "#         X_ = F.relu(self.fc1(X_))\n",
    "#         X_ = F.relu(self.fc2(X_))\n",
    "#         X_ = self.fc3(X_)\n",
    "        \n",
    "#         return X_\n",
    "#         X = torch.tanh(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "\n",
    "        X = self.NMR.normalize_input(X)\n",
    "    \n",
    "        X = torch.relu(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "        X = torch.relu(torch.matmul(self.W1, X) + self.b1)\n",
    "        X = torch.relu(torch.matmul(self.W2, X) + self.b2)\n",
    "        X = torch.matmul(self.W3, X) + self.b3\n",
    "        return self.NMR.denormalize_output(X.t())\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y, y_hat)\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "        \n",
    "        sample_count = X.shape[0]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[idx_rand, :])\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "        \n",
    "        return (epoch_num, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f246ce-9832-420f-8d15-b4d669c5d50e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### FC with TCC Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f4371-8ebb-4b30-b4d9-699ee3c88898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_FC_Normalized_TCC:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 14\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 10000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        h1 = 20\n",
    "        h2 = 40\n",
    "        h3 = 20\n",
    "        \n",
    "        W0_rand = 1/np.sqrt(self.input_size)\n",
    "        W0_init = np.random.uniform(-W0_rand, W0_rand, [h1, self.input_size])\n",
    "\n",
    "        b0_rand = 1/np.sqrt(1)\n",
    "        b0_init = np.random.uniform(-b0_rand, b0_rand, [h1, 1])\n",
    "\n",
    "        W1_rand = 1/np.sqrt(h1)\n",
    "        W1_init = np.random.uniform(-W1_rand, W1_rand, [h2, h1])\n",
    "\n",
    "        b1_rand = 1/np.sqrt(1)\n",
    "        b1_init = np.random.uniform(-b1_rand, b1_rand, [h2, 1])\n",
    "        \n",
    "        W2_rand = 1/np.sqrt(h2)\n",
    "        W2_init = np.random.uniform(-W1_rand, W1_rand, [h3, h2])\n",
    "\n",
    "        b2_rand = 1/np.sqrt(1)\n",
    "        b2_init = np.random.uniform(-b1_rand, b1_rand, [h3, 1])\n",
    "        \n",
    "        W3_rand = 1/np.sqrt(h3)\n",
    "        W3_init = np.random.uniform(-W1_rand, W1_rand, [self.output_size, h3])\n",
    "\n",
    "        b3_rand = 1/np.sqrt(1)\n",
    "        b3_init = np.random.uniform(-b1_rand, b1_rand, [self.output_size, 1])\n",
    "\n",
    "        self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "        self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "        self.W1 = torch.tensor(W1_init, requires_grad=True)\n",
    "        self.b1 = torch.tensor(b1_init, requires_grad=True)\n",
    "        self.W2 = torch.tensor(W2_init, requires_grad=True)\n",
    "        self.b2 = torch.tensor(b2_init, requires_grad=True)\n",
    "        self.W3 = torch.tensor(W3_init, requires_grad=True)\n",
    "        self.b3 = torch.tensor(b3_init, requires_grad=True)\n",
    "#         self.fc1 = nn.Linear(self.input_size, h1)\n",
    "#         self.fc2 = nn.Linear(h1, h2)\n",
    "#         self.fc3 = nn.Linear(h2, self.output_size)\n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            [\n",
    "                self.W0, self.b0,\n",
    "                self.W1, self.b1,\n",
    "                self.W2, self.b2,\n",
    "                self.W3, self.b3\n",
    "            ],\n",
    "#             [\n",
    "#                 self.fc1.weight, self.fc1.bias,\n",
    "#                 self.fc2.weight, self.fc2.bias,\n",
    "#                 self.fc3.weight, self.fc3.bias,\n",
    "#             ],\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        # W1 * (relu(W0 * X + b0)) + b1\n",
    "        \n",
    "#         X_ = X.clone().type(torch.FloatTensor)\n",
    "        \n",
    "#         X_ = F.relu(self.fc1(X_))\n",
    "#         X_ = F.relu(self.fc2(X_))\n",
    "#         X_ = self.fc3(X_)\n",
    "        \n",
    "#         return X_\n",
    "#         X = torch.tanh(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "\n",
    "        X = self.NMR.normalize_input(X)\n",
    "    \n",
    "        X = torch.relu(torch.matmul(self.W0, X.t()) + self.b0)\n",
    "        X = torch.relu(torch.matmul(self.W1, X) + self.b1)\n",
    "        X = torch.relu(torch.matmul(self.W2, X) + self.b2)\n",
    "        X = torch.matmul(self.W3, X) + self.b3\n",
    "        return self.NMR.denormalize_output(X.t())\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y, y_hat)\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "        \n",
    "        sample_count = X.shape[0]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size)) * self.batch_size:\n",
    "                X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[idx_rand, :])\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "        \n",
    "        return (epoch_num, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1c098-7138-4fc3-b6fc-2295ca9ca477",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33ab20-c8aa-4d35-adb5-34459bff87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_Linear_Normalized:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 13\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 50000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        W0_rand = 1/np.sqrt(self.input_size)\n",
    "        W0_init = np.random.uniform(-W0_rand, W0_rand, [self.output_size, self.input_size])\n",
    "\n",
    "        b0_rand = 1/np.sqrt(1)\n",
    "        b0_init = np.random.uniform(-b0_rand, b0_rand, [self.output_size, 1])\n",
    "\n",
    "        self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "        self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            [\n",
    "                self.W0, self.b0\n",
    "            ],\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.NMR.normalize_input(X)\n",
    "    \n",
    "        X = torch.matmul(self.W0, X.t()) + self.b0\n",
    "        \n",
    "        return self.NMR.denormalize_output(X.t())\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y, y_hat)\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "        \n",
    "        sample_count = X.shape[0]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size))* self.batch_size:\n",
    "                X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[idx_rand, :])\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "        \n",
    "        return (epoch_num, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50134832-fda5-4ff4-b612-04df02453ec1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Linear with TCC Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251523ad-1159-4bb6-875e-ea51bbe7dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTCC_Model_Linear_Normalized_TCC:\n",
    "    def __init__(self):\n",
    "        self.NMR = utl.Normalizer()\n",
    "        \n",
    "        self.input_size = 14\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 50000\n",
    "        self.step_size = 1e-3\n",
    "        \n",
    "        W0_rand = 1/np.sqrt(self.input_size)\n",
    "        W0_init = np.random.uniform(-W0_rand, W0_rand, [self.output_size, self.input_size])\n",
    "\n",
    "        b0_rand = 1/np.sqrt(1)\n",
    "        b0_init = np.random.uniform(-b0_rand, b0_rand, [self.output_size, 1])\n",
    "\n",
    "        self.W0 = torch.tensor(W0_init, requires_grad=True)\n",
    "        self.b0 = torch.tensor(b0_init, requires_grad=True)\n",
    "        \n",
    "        self.optim = torch.optim.Adam(\n",
    "            [\n",
    "                self.W0, self.b0\n",
    "            ],\n",
    "            lr=self.step_size\n",
    "        )\n",
    "        \n",
    "    def foward(self, X):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like tensor\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.NMR.normalize_input(X)\n",
    "    \n",
    "        X = torch.matmul(self.W0, X.t()) + self.b0\n",
    "        \n",
    "        return self.NMR.denormalize_output(X.t())\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        y := [TCC],                  (sample_size, 1) like tensor\n",
    "        y_hat := [TCC],              (sample_size, 1) like tensor\n",
    "        \"\"\"\n",
    "        return F.mse_loss(y, y_hat)\n",
    "    \n",
    "    def train(self, X, y_hat, max_epoch):\n",
    "        \"\"\"\n",
    "        X := [LTS, SST, Subsidence], (sample_size, 4) like matrix\n",
    "        y_hat := [TCC],              (sample_size, 1) like matrix\n",
    "        \"\"\"\n",
    "        progress_bar = utl.TimedProgressBar(max_epoch).update(msg=\"Initialization\")\n",
    "        \n",
    "        self.NMR.set_mean_and_sd(torch.tensor(X), torch.tensor(y_hat))\n",
    "        \n",
    "        sample_count = X.shape[0]\n",
    "        \n",
    "        epoch_num = []\n",
    "        losses = []\n",
    "        \n",
    "        for i in np.arange(max_epoch):\n",
    "            for idx_batch in np.arange(int(sample_count / self.batch_size))* self.batch_size:\n",
    "                X_batch = torch.tensor(X[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_hat_batch = torch.tensor(y_hat[idx_batch:idx_batch + self.batch_size, :])\n",
    "                y_batch = self.foward(X_batch)\n",
    "                loss = self.loss(y_batch, y_hat_batch)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "            idx_rand = np.random.randint(0, sample_count, [self.batch_size])\n",
    "            X_batch = torch.tensor(X[idx_rand, :])\n",
    "            y_hat_batch = torch.tensor(y_hat[idx_rand, :])\n",
    "            y_batch = self.foward(X_batch)\n",
    "            loss = self.loss(y_batch, y_hat_batch)\n",
    "            \n",
    "            epoch_num.append(i)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.update(i + 1, \"epoch {0}\".format(i))\n",
    "        \n",
    "        return (epoch_num, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4da0f3-62b1-4fea-ba7e-921adde9abd2",
   "metadata": {},
   "source": [
    "***\n",
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d7007-9ecb-4e4e-aaeb-3c9f0ac927a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_procedure():\n",
    "    ### Data\n",
    "\n",
    "    (idx_training_set, idx_test_set) = get_training_and_testing_set()\n",
    "\n",
    "\n",
    "    ### Training\n",
    "\n",
    "    #### GRU\n",
    "    print(\"---------------------- GRU ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    tcc_model_gru = MLTCC_Model_GRU_Normalized()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_gru.train_time_series(X_train, y_hat_train, 50, 2)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_gru.foward(tsr(X_test)).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU\")\n",
    "\n",
    "    # for idx in np.arange(9):\n",
    "    #     plot_y_fit = tcc_model_gru.foward(tsr(X_test[:idx+1])).detach().numpy()[:, 0]\n",
    "    #     plot_y_hat = y_hat_test[idx][:, 0]\n",
    "    #     plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"T{0}\".format(idx))\n",
    "\n",
    "    # y_fit_series = []\n",
    "    # for idx in np.arange(9):\n",
    "    #     y_fit_series.append(tcc_model_gru.foward(tsr(X_test[:idx+1])).detach().numpy())\n",
    "\n",
    "    # y_fit_series = np.array(y_fit_series)\n",
    "\n",
    "    # for i in np.arange(10):\n",
    "    #     idx = np.random.randint(0, y_hat_test.shape[1])\n",
    "    #     plot(\n",
    "    #         np.arange(9), \n",
    "    #         (y_fit_series[:, idx, 0], y_hat_test[:, idx, 0]), \n",
    "    #         x_label=\"T\", \n",
    "    #         y_label=\"TCC %\", \n",
    "    #         legend=[\"fit\", \"true\"], \n",
    "    #         title=\"Time Series Prediction, Sample: {0}\".format(idx)\n",
    "    #     )\n",
    "\n",
    "    #### GRU with Feedback\n",
    "    print(\"\\n---------------------- GRU with Feedback ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    tcc_model_gru_TCC = MLTCC_Model_GRU_Normalized_TCC()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_gru_TCC.train_time_series(X_train, y_hat_train, 50, 2)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_gru_TCC.foward(tsr(X_test)).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU with TCC Feedback Prediction\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU with TCC Feedback Prediction\")\n",
    "\n",
    "    # for idx in np.arange(9):\n",
    "    #     plot_y_fit = tcc_model_gru_TCC.foward(tsr(X_test[:idx+1])).detach().numpy()[:, 0]\n",
    "    #     plot_y_hat = y_hat_test[idx][:, 0]\n",
    "    #     plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"T{0}\".format(idx))\n",
    "\n",
    "    # y_fit_series = []\n",
    "    # for idx in np.arange(9):\n",
    "    #     y_fit_series.append(tcc_model_gru_TCC.foward(tsr(X_test[:idx+1])).detach().numpy())\n",
    "\n",
    "    # y_fit_series = np.array(y_fit_series)\n",
    "\n",
    "    # for i in np.arange(10):\n",
    "    #     idx = np.random.randint(0, y_hat_test.shape[1])\n",
    "    #     plot(\n",
    "    #         np.arange(9), \n",
    "    #         (y_fit_series[:, idx, 0], y_hat_test[:, idx, 0]), \n",
    "    #         x_label=\"T\", \n",
    "    #         y_label=\"TCC %\", \n",
    "    #         legend=[\"fit\", \"true\"], \n",
    "    #         title=\"Time Series Prediction, Sample: {0}\".format(idx)\n",
    "    #     )\n",
    "\n",
    "    #### FC\n",
    "    print(\"\\n---------------------- FC ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_FC = flatten_time_axis(X_train)\n",
    "    y_hat_train_FC = flatten_time_axis(y_hat_train)\n",
    "    X_test_FC = flatten_time_axis(X_test)\n",
    "    y_hat_test_FC = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    tcc_model_FC = MLTCC_Model_FC_Normalized()\n",
    "\n",
    "    # (x_axis, losses) = tcc_model_FC.train(X_train_FC, y_hat_train_FC, 100)\n",
    "    (x_axis, losses) = tcc_model_FC.train(X_train_FC, y_hat_train_FC, 2000)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    tcc_model_FC.loss(tcc_model_FC.foward(tsr(X_test_FC)), tsr(y_hat_test_FC)).item()\n",
    "\n",
    "    tcc_model_FC.loss(tsr(np.random.randint(0, 100, [y_hat_test_FC.shape[0], 1])), tsr(y_hat_test_FC)).item()\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_FC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network Prediction\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network Prediction\")\n",
    "\n",
    "    #### FC with Feedback\n",
    "    print(\"\\n---------------------- FC with Feedback ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_FC = flatten_time_axis(X_train)\n",
    "    y_hat_train_FC = flatten_time_axis(y_hat_train)\n",
    "    X_test_FC = flatten_time_axis(X_test)\n",
    "    y_hat_test_FC = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    tcc_model_FC_TCC = MLTCC_Model_FC_Normalized_TCC()\n",
    "\n",
    "    # (x_axis, losses) = tcc_model_FC_TCC.train(X_train_FC, y_hat_train_FC, 100)\n",
    "    (x_axis, losses) = tcc_model_FC_TCC.train(X_train_FC, y_hat_train_FC, 2000)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    tcc_model_FC.loss(tcc_model_FC_TCC.foward(tsr(X_test_FC)), tsr(y_hat_test_FC)).item()\n",
    "\n",
    "    tcc_model_FC.loss(tsr(np.random.randint(0, 100, [y_hat_test_FC.shape[0], 1])), tsr(y_hat_test_FC)).item()\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_FC_TCC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network with TCC Feedback\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network with TCC Feedback\")\n",
    "\n",
    "    #### Linear\n",
    "    print(\"\\n---------------------- Linear ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_Linear = flatten_time_axis(X_train)\n",
    "    y_hat_train_Linear = flatten_time_axis(y_hat_train)\n",
    "    X_test_Linear = flatten_time_axis(X_test)\n",
    "    y_hat_test_Linear = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    tcc_model_Linear = MLTCC_Model_Linear_Normalized()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_Linear.train(X_train_Linear, y_hat_train_Linear, 500)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    tcc_model_Linear.loss(tcc_model_Linear.foward(tsr(X_test_Linear)), tsr(y_hat_test_Linear)).item()\n",
    "\n",
    "    tcc_model_Linear.loss(tsr(np.random.randint(0, 100, [y_hat_test_Linear.shape[0], 1])), tsr(y_hat_test_Linear)).item()\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_Linear.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    # # idx_sample = np.random.choice(np.arange(X_test_Linear.shape[0]), [1000], replace=False)\n",
    "    # idx_sample = np.arange(X_test_Linear.shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # # transform them from shape[n:1] into shape[n]\n",
    "    # plot_y_fit = tcc_model_Linear.foward(tsr(X_test_Linear[idx_sample])).detach().numpy()[:, 0]\n",
    "    # plot_y_hat = y_hat_test_Linear[idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction\")\n",
    "\n",
    "    # print(\"Weights: {0}\\nOffsets: {1}\".format(tcc_model_Linear.W0, tcc_model_Linear.b0))\n",
    "\n",
    "    #### Linear with Feedback\n",
    "    print(\"\\n---------------------- Linear with Feedback ----------------------\")\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_Linear = flatten_time_axis(X_train)\n",
    "    y_hat_train_Linear = flatten_time_axis(y_hat_train)\n",
    "    X_test_Linear = flatten_time_axis(X_test)\n",
    "    y_hat_test_Linear = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    tcc_model_Linear_TCC = MLTCC_Model_Linear_Normalized_TCC()\n",
    "\n",
    "    (x_axis, losses) = tcc_model_Linear_TCC.train(X_train_Linear, y_hat_train_Linear, 500)\n",
    "\n",
    "    # plot(x_axis, losses, \"epoch\", \"losses\", \"loss\", \"epoch vs. losses\")\n",
    "\n",
    "    losses[-1]\n",
    "\n",
    "    tcc_model_Linear_TCC.loss(tcc_model_Linear_TCC.foward(tsr(X_test_Linear)), tsr(y_hat_test_Linear)).item()\n",
    "\n",
    "    tcc_model_Linear_TCC.loss(tsr(np.random.randint(0, 100, [y_hat_test_Linear.shape[0], 1])), tsr(y_hat_test_Linear)).item()\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_Linear_TCC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    # # idx_sample = np.random.choice(np.arange(X_test_Linear.shape[0]), [1000], replace=False)\n",
    "    # idx_sample = np.arange(X_test_Linear.shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # # transform them from shape[n:1] into shape[n]\n",
    "    # plot_y_fit = tcc_model_Linear.foward(tsr(X_test_Linear[idx_sample])).detach().numpy()[:, 0]\n",
    "    # plot_y_hat = y_hat_test_Linear[idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction with TCC Feedback\")\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction with TCC Feedback\")\n",
    "\n",
    "    # print(\"Weights: {0}\\nOffsets: {1}\".format(tcc_model_Linear.W0, tcc_model_Linear.b0))\n",
    "\n",
    "    ### Report\n",
    "\n",
    "    r_value = {}\n",
    "\n",
    "    y_data_record = {}\n",
    "\n",
    "    plot_y_hat = y_hat_test[-1][:, 0]\n",
    "    y_data_record[\"y_hat\"] = plot_y_hat\n",
    "\n",
    "    #### GRU\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_gru.foward(tsr(X_test)).detach().numpy()[:, 0]\n",
    "\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU\")\n",
    "\n",
    "    r_value[\"GRU\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"GRU\"] = plot_y_fit\n",
    "\n",
    "    #### GRU with Feedback\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_gru_TCC.foward(tsr(X_test)).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"GRU with TCC Feedback Prediction\")\n",
    "\n",
    "    r_value[\"GRU_FB\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"GRU_FB\"] = plot_y_fit\n",
    "\n",
    "    #### FC\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_FC = flatten_time_axis(X_train)\n",
    "    y_hat_train_FC = flatten_time_axis(y_hat_train)\n",
    "    X_test_FC = flatten_time_axis(X_test)\n",
    "    y_hat_test_FC = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_FC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network Prediction\")\n",
    "\n",
    "    r_value[\"FC\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"FC\"] = plot_y_fit\n",
    "\n",
    "    #### FC with Feedback\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_FC = flatten_time_axis(X_train)\n",
    "    y_hat_train_FC = flatten_time_axis(y_hat_train)\n",
    "    X_test_FC = flatten_time_axis(X_test)\n",
    "    y_hat_test_FC = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_FC_TCC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Fully Connected Neural Network with TCC Feedback\")\n",
    "\n",
    "    r_value[\"FC_FB\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"FC_FB\"] = plot_y_fit\n",
    "\n",
    "    #### Linear\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_Linear = flatten_time_axis(X_train)\n",
    "    y_hat_train_Linear = flatten_time_axis(y_hat_train)\n",
    "    X_test_Linear = flatten_time_axis(X_test)\n",
    "    y_hat_test_Linear = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_Linear.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction\")\n",
    "\n",
    "    r_value[\"Linear\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"Linear\"] = plot_y_fit\n",
    "\n",
    "    #### Linear with Feedback\n",
    "\n",
    "    ((X_train, y_hat_train), (X_test, y_hat_test)) = get_data_TCC_feedback(idx_test_set, idx_training_set)\n",
    "\n",
    "    X_train_Linear = flatten_time_axis(X_train)\n",
    "    y_hat_train_Linear = flatten_time_axis(y_hat_train)\n",
    "    X_test_Linear = flatten_time_axis(X_test)\n",
    "    y_hat_test_Linear = flatten_time_axis(y_hat_test)\n",
    "\n",
    "    # idx_sample = np.random.choice(np.arange(X_test_FC.shape[0]), [1000], replace=False)\n",
    "    idx_sample = np.arange(X_test[-1].shape[0])  # All sample\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = tcc_model_Linear_TCC.foward(tsr(X_test[-1][idx_sample])).detach().numpy()[:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][idx_sample][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"Linear Prediction with TCC Feedback\")\n",
    "\n",
    "    r_value[\"Linear_FB\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    y_data_record[\"Linear_FB\"] = plot_y_fit\n",
    "\n",
    "    print(\"\\n---------------------- ERA5 ----------------------\")\n",
    "\n",
    "    df = utl.read_time_series_data(\"full_rank_dataset_ERA5\")\n",
    "\n",
    "    idx_test_set = np.random.choice(np.arange(df[0].shape[0]), [int(0.1 * df[0].shape[0])], False)\n",
    "    idx_training_set = np.delete(np.arange(df[0].shape[0]), idx_test_set)\n",
    "\n",
    "    time_arr = np.arange(9)\n",
    "\n",
    "    X_full = [np.c_[df[time]['ERA5'].to_numpy()] for time in time_arr]\n",
    "    y_hat_full = [np.c_[df[time]['TCC'].to_numpy()] for time in time_arr]\n",
    "\n",
    "    X_train = np.array([X_full[time][idx_training_set] for time in time_arr])\n",
    "    y_hat_train = np.array([y_hat_full[time][idx_training_set] for time in time_arr])\n",
    "\n",
    "    X_test = np.array([X_full[time][idx_test_set] for time in time_arr])\n",
    "    y_hat_test = np.array([y_hat_full[time][idx_test_set] for time in time_arr])\n",
    "\n",
    "    # transform them from shape[n:1] into shape[n]\n",
    "    plot_y_fit = X_test[-1][:, 0]\n",
    "    plot_y_hat = y_hat_test[-1][:, 0]\n",
    "\n",
    "    plot_diff_percentage_bined_average(plot_y_hat, plot_y_fit, \"true\", \"fit\", \"ERA5 Prediction\")\n",
    "\n",
    "    r_value[\"ERA5\"] = calc_r_value(plot_y_hat, plot_y_fit)\n",
    "    \n",
    "    y_data_record[\"y_hat_ERA5\"] = plot_y_hat\n",
    "    y_data_record[\"ERA5\"] = plot_y_fit\n",
    "\n",
    "    #### save data\n",
    "    print(\"Save to File: {0}\".format(\n",
    "        save_result(\"{0}{1}{2}\".format(OUTPUT_FOLDER_PATH, OUTPUT_FILE_PREFIX, utl.get_current_day_time_string()), y_data_record)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c40d34-b36c-481b-a8a3-27f21bfe5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(100):\n",
    "    T = utl.get_runtime_marker()\n",
    "    benchmark_procedure()\n",
    "    print(\"EPOCH {0} FINISHED, TIME: {1}\".format(i + 1, utl.format_time_s_2_hms(utl.get_runtime_in_second(T))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e319d-43bd-4b86-92be-76c1fdb68f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
